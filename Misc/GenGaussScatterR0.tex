\documentclass[journal]{IEEEtran}

% *** GRAPHICS RELATED PACKAGES ***
\ifCLASSINFOpdf
\else
\fi

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{graphicx}
\graphicspath{{../Figures/}}

\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{bm}

\usepackage{mathtools}
\usepackage[justification=centering]{caption}
\usepackage[justification=centering,font=footnotesize]{subcaption}
\usepackage{upgreek}
\usepackage{cite}
\usepackage{enumerate}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{flushend}

\begin{document}
\title{A Generalized Gaussian Coherent Scatterer \\ Model for
Correlated SAR Texture}
\author{\IEEEauthorblockN{Dong-Xiao Yue\IEEEauthorrefmark{1},~\IEEEmembership{Student Member,~IEEE},
Feng Xu\IEEEauthorrefmark{1},~\IEEEmembership{Senior Member,~IEEE},
Alejandro C.\ Frery\IEEEauthorrefmark{2},~\IEEEmembership{Senior Member,~IEEE}, and
Ya-Qiu Jin\IEEEauthorrefmark{1},~\IEEEmembership{Fellow,~IEEE}}

\thanks{Manuscript received XXX; revised XXX. This work was supported in part by National Key R\&D Program of China no. 2017YFB0502703 and Natural Science Foundation of China no. 61822107, 61571134.

Dong-Xiao Yue, Feng Xu and Ya-Qiu Jin are with the Key Lab for Information Science of Electromagnetic Waves (MoE),
Fudan University, Shanghai 200433, China(email: fengxu@fudan.edu.cn).

Alejandro C.\ Frery is with the Universidade Federal de Alagoas (Ufal), Laborat\'orio de Computa\c{c}\~{a}o Cient\'ifica e An\'alise Num\'erica (LaCCAN), Macei\'{o}, AL, Brazil, and with the Key Lab of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China.
}}

\bibliographystyle{unsrt}

% The paper headers
\markboth{IEEE Transactions on Geoscience and Remote Sensing}%
{Yue \MakeLowercase{\textit{et al.}}: Generalized Scatterer Model for Correlated SAR Texture}

\IEEEtitleabstractindextext{%
\begin{abstract}
	This paper proposes a generalized modeling and simulation approach for correlated SAR texture based on the Gaussian coherent scatterer model.
	It is rooted in the physics-based coherent scatterer assumption where each observation in a SAR image is a coherent sum of multiple underlying Gaussian scatterers.
	The proposal generalizes existing single-point statistical models by allowing the number of scatterers to be a correlated random field.
	It can also generate the desired spatial correlation texture by stipulating the structure in both the Gaussian scattered field and the number of scatterers.
	This generalized model is derived theoretically and then validated by both simulations and experiments with SAR data from actual sensors.
\end{abstract}

\begin{IEEEkeywords}
	SAR statistical modeling,
	Coherent scatterer model,
	Correlated SAR texture.
\end{IEEEkeywords}}

\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle

\section{Introduction}

\IEEEPARstart{S}{tatistical} modeling of SAR images is the theoretical basis of SAR image processing and interpretation: image filtering, image classification, object detection, and other operations rely on tractable and expressive models.
A good statistical model should be an effective and efficient representation of the often complicated SAR data.
It mainly involves two aspects: the single-point distribution of image pixel intensities, and two-point statistical relations, i.e. the correlation function, which characterizes image texture information.

Ref.~\cite{Yue:2019b} reviews the mainstream models of SAR image statistical modeling and summarizes more than twenty single-point distributions for different physical scenes, as well as some correlation function models.
It is found that the existing statistical models have three drawbacks, namely,
\begin{enumerate}
\item most are phenomenological methods which do not take into account the actual physical process of scattering;
\item each statistical model is usually suitable for only one type of specific scenario of terrain surface; and
\item most of the models do not take into account the spatial correlation along with the single-point distribution.
\end{enumerate}

To address these issues, this study establishes a new framework for SAR statistical modeling with the following properties: 1)~it is originated from a physics-plausible assumption, i.e. the coherent scattering process of radar waves; 2)~it is adaptable to various scenarios, i.e. a generalized framework under which most existing models can be included as a special case; and 3)~it is also able to describe the 2D spatial texture features of SAR images.

Sant'Anna et al.~\cite{Sant:2008} proposed an electromagnetic approach for the simulation of polarimetric SAR images where image pixels are modeled as the vector summation of far-field scattered by multilayer structures of dipole elementary scatterers.
The authors assumed independent dipole scatterers and computed its far-field via the moment method.
The coherent scatterer~\cite{Oliver:2004} can be seen as a statistical realization of the electromagnetic simulation model.
The electric field of the individual scatterer is represented by a complex random variable.
It models the image pixel as a coherent sum of many complex-valued components with independent phases.

Deng et al.~\cite{Deng:RandomWalk2016} designed a simulator for polarimetric SAR clutter from a physical point of view using the coherent scatterer model.
They incorporated the influences of the scatterer type and scatterer number on the statistical characteristics of SAR images.
The authors addressed
Gaussian scatterers,
K-scatterers,
Beta-scatterers and
mixture-of-scatterers.
They modeled the number of scatterers as:
constant,
with a negative binomial distribution,
and with a Poisson distribution with varying mean.
According to~\cite{Deng:RandomWalk2016}, SAR speckle can be interpreted in two different ways, that is 1)~speckle is either caused by the fluctuation of the number of scatterers per resolution cell, or
2)~caused by the mixture of different scatterer types. It is difficult to distinguish between these two interpretations.
Besides, the simulation processes for SAR data in different statistical distributions are different which limits its application. Moreover, the spatial correlation is ignored in~\cite{Deng:RandomWalk2016}.
%%% ACF Critical! What is the difference between our paper and the one by Deng et al.? This is the place to make the contribution clear. Use the term "random walk" to refer to their paper.

%%% Yue The difference between our paper and Deng et al. are: 1)the Gaussian assumption in our work, this assumption simplify the analysis and avoids the confusion between scatterer type and the number of scatterers. 2)Though Deng's work gives a physical analysis, they simulate SAR images with different distributions separately. However, our work gives a generalized framework for different distributions.3) Deng's work didn't consider the spatial correlation. 4) An advantage of Deng's work is reflected on the polarimetric data while our work is only suitable for single-channel data.  This difference is added in Line 103. Also, the term "random walk" is added to refer to their paper.

In this paper, such a generalized framework is proposed based on the Gaussian coherent scatterer model.
It is in itself a physics-based model but with the assumption that all scatterers are Gaussian.
This simplification avoids the confusion between scatterer type and varying the number of scatterers at the cost of a single scatterer type.
%%% ACF Complete the previous sentence with "at the cost of..."
%%% Yue Do you mean why we can avoid the confusion between scatterer type and varying the number scatterers? Because there is only one kind of scatterer type are assumed here--Gaussian scatterer. Therefore I complete the previous sentence with"at the cost of a single scatterer type".

%%% ACF Also, we can stipulate the spatial correlation. Does the previous paper go in this direction too?
%%% Yue The previous paper doesn't consider the spatial correlation.
The proposed framework generalizes existing single-point distribution models by stipulating the distribution of the number of scatterers in each pixel.
On one hand, the generalized framework can represent a variety of single-point distribution models including the well-known Rayleigh, $K$, $G^0$, $W$, and $U$ distributions.
On the other hand, the adoption of the Gaussian coherent scatterer model enables us to establish analytical relationships between the parameters of underlying scatterers and the statistical properties of the image.

Under this framework, the scattered field of each pixel is modeled as the coherent sum of multiple underlying scatterers, which has clear physical meaning.
Most importantly, it can model the texture of SAR image by incorporating the spatial correlation function into the underlying Gaussian scatterers.
Besides providing a means for understanding and checking the properties of observed data, it is a tool for simulating realistic data that can be used as input for training Neural Networks.

In this paper, we present both the theoretical description and properties of the generalized framework and experimental results of the correlated SAR texture simulation.
Section~\ref{Sec:GGCSModel} introduces the generalized Gaussian coherent scatterer model.
In Section~\ref{Sec:Distributions} we present the theoretical representation of eight probability distributions based on the generalized framework and their experimental validation with a Monte Carlo study.
Section~\ref{Sec:Texture} derives the theoretical texture modeling under the Generalized framework.
Section~\ref{Sec:Simulation} validates the proposed generalized framework by using simulated and real SAR data.
Section~\ref{Sec:Conclusions} presents the conclusions.
The Appendix provides detailed derivations.
%%% ACF If the distributions of the Poisson random variable whose mean fluctuates according to the Inverse Gamma etc. distributions have not seen the light, it would be interesting to add another appendix with these results.
%%% Yue The distributions of the Poisson random variable whose mean fluctuates according to the Inverse Gamma、the first kind of beta and the second kind of beta etc. distributions have not seen the light. There need further derivation for the first kind of beta and the second kind of beta, Maybe we can add these later.
\section{Generalized Gaussian Coherent Scatterer Model}\label{Sec:GGCSModel}

The coherent scatterer model, also known as discrete scatterer model~\cite{Oliver:2004}, describes the speckle phenomenon caused by the coherent sum of a large number of complex-valued components with independent phases.
Such a model describes the complex return from a single image cell as:
%%% ACF Critical! I changed the notation: capital letters for random variables; lowercase for their events
%%% Yue The notation of a_i is changed to A_i, this change will bring changes and confusion with the following notation, such as Eq.(2) and Eq.(9), please take some attention.
\begin{align}
\boldsymbol{A} & =A e^{j \phi}=\mathcal{R}+j \mathcal{I}=\sum_{i=1}^{N} \boldsymbol{a}_{i} \nonumber \\
& =\sum_{i=1}^{N} a_{i} e^{j \phi_{i}}=\sum_{i=1}^{N}\left(\mathcal{R}_{i}+j \mathcal{I}_{i}\right),
\label{eq1:CoherentScatterer}
\end{align}
where
$A$ is the amplitude of the received signal,
$\phi$ its phase,
$j=\sqrt{-1}$, and
$\mathcal R$ and $\mathcal I$ the decomposition of $\boldsymbol{A}$ in its real and imaginary parts.
These components are the coherent sum of the electromagnetic scattered fields from $N$ independent scatterers, each with amplitude $a_i$ and phase $\phi_i$.
Eq.~\eqref{eq1:CoherentScatterer} shows that the statistical properties of the scattered field $\boldsymbol{A}$ are completely determined by the statistical properties of $\mathcal{R}_{i}$, $\mathcal{I}_{i}$ and $N$.

%%% ACF: Critical!
%%% At this point we should make it clear that these are the basic assumptions, then state which paper relaxed wich, and conclude by saying which are the ones we relax here. Notice that I added the first assumption.
%%% Yue The first assumption is very important and this assumption brings the important Rayleigh distribution. This completes this section!
The following hypotheses simplify this model:
\begin{enumerate}[{H}-1)]
\item\label{NInfty} There are infinitely many backscatterers in the resolution cell: $N\to\infty$.
\item\label{Indep} Amplitudes and phases are collectively independent.
\item\label{Uniform} Each phase component $\phi_{i}$ obeys a uniform distribution on $(-\pi,\pi]$.
\item\label{NoDominant} There are no dominant scatterers in a resolution cell.
\end{enumerate}
Goodman~\cite{ChapterGoodman} used these hypotheses and obtained a Rayleigh distribution for $\|\boldsymbol{A}\|$ or, equivalently, an Exponential distribution for the intensity $\|\boldsymbol{A}\|^2$.
This result stems from the Central Limit Theorem (CLT).
Eq.~\eqref{eq1:CoherentScatterer} is the starting point of our work.

Although hypotheses~\ref{Indep}--\ref{NoDominant} can be relaxed to some extent, cf.\ Ref.~\cite[Chapter 2]{ProbabilityTheoryandExamples2005}, hypothesis~\ref{NInfty} is required in order to the CLT hold.
Hypothesis~\ref{NInfty}, however, is not realistic in many practical situations where there are a few backscatterers per resolution cell, as is the case of high-resolution SAR sensors.

According to Kuruo\u glu and Zerubia~\cite{Kuruoglu:2004}, it is possible to invoke a Generalized CLT, and assume that $(\mathcal{R}, \mathcal{I})$ are independent $\upalpha$-stable distributed random variables.
This hypothesis leads to a heavy-tailed Generalized Rayleigh distribution.
Although this model provides a good fit to data, an explicit form of its density is not available in general.

It is impractical to require both the knowledge of $N$ and the joint distribution of $a_1,\dots,a_N$ and $\phi_1,\dots,\phi_N$ to obtain the distribution of $\boldsymbol{A}$. Another venue for obtaining a more general model than the one provided by hypotheses~\ref{NInfty}--\ref{NoDominant} consists in assuming that $N$ obeys a Poisson distribution with parameter $\Lambda$.
For instance,
Delignon and Pieczynski~\cite{Delignon:2002}
obtained the $K$, $G^0$, $U$ and $W$ distributions assuming that $\Lambda$ follows Pearson-type distributions with positive support.
This approach could model several real data scenes and it inspires our modeling of the number of scatterers.
%%% ACF Any other generalization of \Lambda worth citing?
%%% Yue Actually the RiIG (Rician inverse Gaussian) distribution ( ref: Eltoft 2017) can obtained by assuming the \Lambda in inverse Gaussian distribution. Though this interpretation is not validated by the strict derivation which is difficult, I validate it by simulation samples. Maybe you can decide if we can relate it here. Actually, this point is also mentioned in Section3.

%%% ACF Here, \upalpha-stable distributions.
%%% Yue Sorry, I don't understand what you mean, do we need to introduce the \upalpha-stable distributions here? Actually it is mentioned in Section3, please see the Section3 as a reference.

%%% ACF Rephrase the first sentence. If necessary, break it into smaller sentences.
Fig.~\ref{Figure1-Distributions} summarizes some typical distribution types of scattered field, including real-imaginary component, amplitude-phase component, and scatterer number.
The scattered field and the number of scatterers are the two major factors to be considered in the coherent scatterer model.

This paper proposes a general model for SAR texture generation where the basic Gaussian coherent scatterer model is adopted.
The Gaussian coherent scatterer model is a simplification
of the coherent scatterer model which assumes that the pair $(\mathcal{R}_{i} , \mathcal{I}_{i})$ obeys an
$\upalpha$-stable law, of which the Gaussian distribution is a special case~\cite{SimulationPropertiesStableLaw}.
The proposed model is generalized in the sense that the number of scatterers is allowed to follow several distributions and, thus, to adapt to different SAR texture models.
Many of the existing statistical models can be regarded as special cases under this generalized framework.
Most importantly, the basic assumption of Gaussian coherent scatter model allows us to easily introduce the two-dimensional (2D) spatial correlation typical of SAR texture.

\begin{figure}[hbt]
\centering
%\includegraphics[width=\linewidth]{Figure1-Distributions}
\caption{Distributions of scattered field and scatterer number}
\label{Figure1-Distributions}
\end{figure}

Fig.~\ref{Figure2-ScatteringScene} illustrates how the model represents a pixel: a 2D region, where $r$ and $c$ denote the range and azimuth dimensions.
The black spots represent individual scatterers with complex-valued scattered field of $\boldsymbol{a}_{r,c}$
\begin{equation}
\boldsymbol{a}_{r, c}=\mathcal{R}_{r, c}+i \mathcal{I}_{r, c} ,
\label{eq2:ScatteredField}
\end{equation}
where $\mathcal{R}_{r, c}$ and $\mathcal{I}_{r, c}$ represent, respectively, the real and imaginary components, and both are Gaussian random variables with mean $\mu$ and variance $\sigma^2$:
\begin{equation}
\begin{split}
\mathcal{R}_{r, c} & \sim N\left(\mu, \sigma^{2}\right), \\
\mathcal{I}_{r, c} & \sim N\left(\mu, \sigma^{2}\right).
\label{eq3:Gaussian}
\end{split}
\end{equation}
To represent the spatial correlation of real SAR images, the 2D region is modeled as a (possibly correlated) Gaussian random field.

\begin{figure}[hbt]
\centering
% Requires \usepackage{graphicx}
%\includegraphics[width=5cm]{Figure2-ScatteringScene}
\caption{Two-dimensional (2D) scattering scene}
\label{Figure2-ScatteringScene}
\end{figure}

As shown in Fig.~\ref{Figure3-GaussianCoherentModel}, a square block represents a resolution cell.
The total received complex scattered field of a resolution cell at position $(r^{\prime},c^{\prime})$
is denoted as $\boldsymbol{A}_{r^{\prime},c^{\prime}}$; and the number of scatterers in the resolution cell is $N_{r^{\prime},c^{\prime} }$, thus:
\begin{align}
\boldsymbol{A}_{r^{\prime}, c^{\prime}} & =\mathcal{R}_{r^{\prime}, c^{\prime}}+j \mathcal{I}_{r^{\prime}, c^{\prime}}=\sum_{i=1}^{N_{r^{\prime}, c^{\prime}}} \boldsymbol{a}_{r_{i}, c_{i}}\nonumber \\
& =\sum_{i=1}^{N_{r^{\prime}, c^{\prime}}}\left(\mathcal{R}_{r_{i}, c_{i}}+j \mathcal{I}_{r_{i} c_{i}}\right),
\label{eq4}
\end{align}
where $\left\{\left(r_{i}, c_{i}\right), i=1,2, \dots, N_{r^{\prime}, c^{\prime}}\right\}$ denotes the set of scatterers located within the resolution cell at position $(r^{\prime},c^{\prime})$.

\begin{figure}[hbt]
\centering
% Requires \usepackage{graphicx}
%\includegraphics[width=9cm]{Figure3-GaussianCoherentModel}
\caption{Gaussian coherent scatterer model.}
\label{Figure3-GaussianCoherentModel}
\end{figure}

By expanding Fig.~\ref{Figure3-GaussianCoherentModel},
Fig.~\ref{Figure4-GeneralizedGaussianModel} depicts the complete proposed generalized Gaussian coherent scatterer model in a three-dimensional (3D) view. The third dimension represents the sequence of scatterers within a resolution cell after 3D expansion.

\begin{figure*}
\centering
% Requires \usepackage{graphicx}
%\includegraphics[width=16 cm]{Figure4-GeneralizedGaussianModel}  \caption{Generalized Gaussian coherent scatterer model}
\label{Figure4-GeneralizedGaussianModel}
\end{figure*}

For a SAR image of size $L \times M$, the maximum number of scatterers per resolution is denoted as $N_{\max}$:
\begin{equation}
N_{\max }=\max \left\{N_{r^{\prime}, c^{\prime}}\right\} ; 1 \leq r^{\prime} \leq L, 1 \leq c^{\prime} \leq M .
\label{eq5}
\end{equation}
Denoting the scattered field of the $i$-th scatterer in the resolution cell at position $(r^{\prime},c^{\prime})$ as $\boldsymbol{A}_{r^{\prime},c^{\prime},i}$, then the total scattered field $\boldsymbol{A}_{r^{\prime},c^{\prime}}$ in the resolution cell is characterized in the $L \times M \times N_{\max}$ 3D space as:
\begin{gather}
\boldsymbol{A}_{r^{\prime},c^{\prime}} =\sum_{i=1}^{N_{r^{\prime}, c^{\prime}}} \boldsymbol{A}_{r^{\prime}, c^{\prime}, i} \\
1 \leq r^{\prime} \leq L, 1 \leq c^{\prime} \leq M, 1 \leq i \leq N_{\max \notag},
\label{eq6}
\end{gather}
where $\boldsymbol{A}_{r^{\prime}, c^{\prime}, i}$ is non-zero value only when there is a scatterer at the position $(r^{\prime}, c^{\prime}, i)$:
\begin{equation}
\boldsymbol{A}_{r^{\prime}, c^{\prime}, i} = \begin{cases}
\boldsymbol{a}_{r_{i}, c_{i}}, & \text { if } 1 \leq i \leq N_{r^{\prime}, c^{\prime}}; \\
0, & \text { if } N_{r^{\prime}, c^{\prime}}<i \leq N_{\max },
\label{eq7}
\end{cases}
\end{equation}
where $\left\{\left(r_{i}, c_{i}\right), i=1,2, \dots, N_{r^{\prime}, c^{\prime}}\right\}$ denotes the set of scatterers located within the resolution cell at position $(r^{\prime}, c^{\prime})$; $\boldsymbol{A}_{r^{\prime}, c^{\prime}, i} = 0$ indicates that there is no scatterer.


The total scattered field of a resolution cell is interpreted, in the 3D representation, as the sum of contributions of multiple underlying Gaussian scatterers.
Similarly, a SAR image in the 2D space can also be interpreted as the sum of multiple 2D Gaussian scattered fields:
\begin{equation}
\boldsymbol{A} = \sum_{i=1}^{N_{\max }} \boldsymbol{A}_{i}, 1 \leq i \leq N_{\max },
\label{eq8}
\end{equation}
where $\boldsymbol{A}_{i}$ represents 2D complex Gaussian random field:
\begin{equation}
\boldsymbol{A}_{i} = \mathcal{R}_{i}+j \mathcal{I}_{i},
\label{eq9}
\end{equation}
and $\mathcal{R}_{i}$ and $\mathcal{I}_{i}$ are the real and imaginary components of the complex Gaussian random field, respectively.

The texture information of a SAR image is caused by the spatial correlation of the distribution of underlying scatterers and scatterers number.
In order to model the texture, both the underlying Gaussian scattered field and scatterers number should be treated as correlated random fields.

The correlated complex Gaussian random field of underlying scatterers can be easily obtained by convolution and linear transformation of an uncorrelated Gaussian random field~\cite{Bustos:2009}, as shown in Fig.~\ref{Figure4-GeneralizedGaussianModel}.
First, a convolution kernel $h_{i}(\tau)$ is convolved with the uncorrelated standard complex Gaussian random field $\boldsymbol{G}_{i}$ to obtain a complex Gaussian random field $\boldsymbol{B}_{i}$ with a correlation coefficient function of $\rho_{i}(\tau)$:
\begin{equation}
\boldsymbol{B}_{i} = \boldsymbol{G}_{i} \circledast h_{i},
\label{eq10}
\end{equation}
where $\circledast$ denotes convolution, and the relationship between the correlation coefficient $\rho_{i}(\tau)$ and the convolution kernel $h_{i}(\tau)$ is:
\begin{equation}
\rho_{i}(\tau)=h_{i}(\tau) \circledast h_{i}^{*}(-\tau),
\label{eq11}
\end{equation}
where the superscript $*$ denotes conjugation.

Then, the correlated complex Gaussian random field $\boldsymbol{A}_{i}$ is obtained through a linear transformation of the complex Gaussian random field $\boldsymbol{B}_{i}$:
\begin{equation}
\boldsymbol{A}_{i}=\mathbf{D}_{i} \cdot \boldsymbol{B}_{i}+\mathbf{E}_{i},
\label{eq12}
\end{equation}
where $\mathbf{D}_{i}$ is a coefficient matrix of size $2 \times 2$, $\mathbf{E}_{i}$ is a constant matrix of size $2 \times 1$, then the relationship between the mean $\mu_{\boldsymbol{B}_{i}}$ and covariance matrix $C_{\boldsymbol{B}_{i}}$ of $\boldsymbol{B}_{i}$ and the mean value $\mu_{\boldsymbol{A}_{i}}$ and covariance matrix $C_{\boldsymbol{A}_{i}}$ of $\boldsymbol{A}_{i}$ is \cite{Conte:1987}:
\begin{equation}
\begin{split}
\mu_{\boldsymbol{A}_{i}} & = \mathbf{D}_{i} \cdot \mu_{\boldsymbol{B}_{i}}+\mathbf{E}_{i} \\
C_{\boldsymbol{A}_{i}} & = \mathbf{D}_{i} C_{\boldsymbol{B}_{i}} \mathbf{D}_{i}^{\mathrm{T}}.
\label{eq13}
\end{split}
\end{equation}

The correlated random field of scatterers number can be obtained by the ITM (Inverse Transform Method) method~\cite{Bustos:2009}.
First, a correlated Gaussian random field is obtained through convolution operation, and then it is converted to the correlated random field with the correct distribution of scatterers number according to their CDF (Cumulative Distribution Function).
The whole process may be implemented by both analytic and numerical methods~\cite{Yue:2019a}.
The theoretical model of texture under the generalized framework will be detailed in Section~\ref{Sec:Texture}.

Finally, according to the Gaussian coherent scatterer model, a SAR image is obtained by the sum of multiple 2D Gaussian fields $\boldsymbol{A}_{i}$.
The SAR image is supposed to satisfy a specific single-point probability distribution and specific correlation characteristics.
The parameters in the generalized model are:
the distribution parameters of the underlying Gaussian scatterers,
the distribution parameters of scatterers number, and
the convolution kernel of the underlying Gaussian field and scatterers number.
The single-point probability distribution of the target SAR image is one-to-one with the parameters of
the Gaussian scatterers and of
the scatterers number.
The convolution kernel of the Gaussian field and of the number of scatterers is one-to-one with the correlation coefficient of the target SAR image.

\section{Single-Point Distribution Under the Generalized Framework}\label{Sec:Distributions}

The parameters of the underlying Gaussian scatterers and of the number of scatterers determine the single-point probability distribution of the SAR image.
The generalized model includes eight traditional probability distribution models under specific parameter settings; Table~\ref{Table1} presents the exact relationship between parameters.

The Rayleigh distribution~\cite{Oliver:2004} describes the scattered field in homogeneous regions, where the number of scatterers is constant and each contribution is Normal.
Alternatively, it also holds when hypotheses~\ref{NInfty}--\ref{NoDominant} are valid.
The S$\upalpha$SGR distribution (heavy-tailed Generalized Rayleigh distribution)~\cite{Kuruoglu:2004} is usually employed to describe the longtailedness amplitude of urban areas; it assumes that the real and imaginary components of the scattered field obey S$\upalpha$S (Symmetric $\upalpha$-stable) distributions~\cite{Zhou:2011}, an assumption based on the Generalized Central Limit Theorem.
Note that Gaussian distribution is a special case of the S$\upalpha$S distribution.
The number of scatterers in S$\upalpha$SGR distribution model is constant too.

The $K$ distribution~\cite{Jakeman:1999} is one of the most commonly used statistical distributions for inhomogeneous regions.
There are three different ways to interpret the $K$ distribution, that we will call ``representations'':
(i)~the number of scatterers $N$ is a random variable following a negative binomial distribution,
(ii)~the number of scatterers $N$ is a random variable following a Poisson law with its mean $\overline{N}$ being itself Gamma distributed, and
(iii)~$N$ is constant, but the variance of the Gaussian random variables is Gamma distributed.

The $G^0$ distribution~\cite{Frery:1997} can be used to describe homogeneous, inhomogeneous, and extremely inhomogeneous regions.
It assumes the number of scatterers $N$ obeys a Poisson distribution with its mean $\overline{N}$ following an Inverse Gamma distribution.

Two other models for inhomogeneous regions can be derived by changing the distribution of $\overline{N}$, namely, the $W$ distribution, in which $\overline{N}$ follows a Beta distribution of the first kind, and the $U$ distribution, in which $\overline{N}$ obeys a Beta law of the second kind~\cite{Delignon:2002}.

The Rician (or Rice) distribution~\cite{Goodman:2007} is used to describe the amplitude probability distribution of a circularly symmetric Gaussian random variable with a non-zero mean.
In the coherent scatterer model, the Rician distribution can be obtained by modeling the total scattered field as a sum of an infinite number of scattered fields and a constant scattering field.
The RiIG (Rician Inverse Gaussian) distribution~\cite{Eltoft:2005} is a generalized form of the Rician law, obtained with non-zero Gaussian random variables.
There are two interpretations for the RiIG distribution:
(i)~assuming that the number of scatterers $N$ is constant, while the variance of the Gaussian scatterers obeys an Inverse Gaussian distribution, and
(ii)~assuming that $N$ is a Poisson random variable with its mean $\overline{N}$ obeying an Inverse Gaussian distribution.

The generalized framework builds on the Gaussian coherent scatterer model which is a sum of multiple Gaussian scatterers, therefore only infinitely divisible models can be represented.
The real and imaginary components of the G$\Gamma$R distribution (Generalized Gamma Rayleigh distribution)~\cite{Li:2010,Li:2011} and GGR distribution (Generalized Gaussian Rayleigh distribution)~\cite{Moser:2006} are modeled as Generalized Gamma distribution and a Generalized Gaussian distribution, respectively; since neither is infinitely divisible, the G$\Gamma$R and GGR laws cannot be represented using the generalized framework.

We validated the theoretical derivations and parameters relationships presented in Table~\ref{Table1} by simulation.
We obtained $10,000$ observations for each distribution, with the parameter values shown in Fig.~\ref{Figure5}.
Fig.~\ref{Figure5} shows
the hypothesized models in red, and
the histograms in blue.
Amplitudes are shown to the left, phases to the right.
Our generalized framework is able to correctly sample from the eight distributions under analysis.

\begin{figure*}[htb]
\centering
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(a)Rayleigh}
	\caption{Rayleigh distribution}\label{Figure5(a)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(b)SaSGR}
	\caption{S$\upalpha$GR distribution}\label{Figure5(b)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(c)K1}
	\caption{$K$ distribution-1}\label{Figure5(c)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(d)K2}
	\caption{$K$ distribution-2}\label{Figure5(d)}
\end{subfigure}
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(e)K3}
	\caption{$K$ distribution-3}\label{Figure5(e)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(f)G0}
	\caption{$G^0$ distribution}\label{Figure5(f)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(g)W}
	\caption{$W$ distribution}\label{Figure5(g)}
\end{subfigure}
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure5-(h)U}
	\caption{$U$ distribution}\label{Figure5(h)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure-5(i)RiIG1}
	\caption{RiIG distribution-1}\label{Figure5(i)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure-5(j)RiIG2}
	\caption{RiIG distribution-2}\label{Figure5(j)}
\end{subfigure}
%  \qquad
\begin{subfigure}[t]{4.2cm}
	\centering
%	\includegraphics[width=4.2cm]{Figure-5(k)Rician}
	\caption{Rician distribution}\label{Figure5(k)}
\end{subfigure}
\caption{Simulated data under the generalized framework, and the theoretical models presented in Table~\ref{Table1} ($N_{m}$ denotes $\overline{N}$, $N_{m}^{\prime}$ denotes the expected value of $\overline{N}$.}\label{Figure5}
\end{figure*}

\section{Texture Modeling Under the Generalized Framework}\label{Sec:Texture}

This section derives analytic relationships among the spatial correlation of SAR image, underlying Gaussian field, and scatterer number.
For simplicity, the problem is abstracted into the following mathematical problem:
the complex scattered field is modeled as a complex random variable $S$
\begin{equation}
S  = \mathcal{R}+j \mathcal{I}=A e^{j \theta}=\sum_{i=1}^{N} z_{i}
=\sum_{i=1}^{N} a_{i} e^{j \phi_{i}}=\sum_{i=1}^{N}\left(x_{i}+j y_{i}\right),
\label{eq14}
\end{equation}
in which $\mathcal{R}=\sum_{i=1}^{N} x_{i}$, and $\mathcal{I}=\sum_{i=1}^{N} y_{i}$,
where $N$ is an integer random variable and $z_i$ is a circular symmetric complex Gaussian random variable;
$x_i$ and $y_i$ are the real and imaginary components of $z_i$, respectively;
$\mathcal{R}$ and $\mathcal{I}$ represent the real and imaginary components of $S$, respectively.
The scattered intensity can then be written as:
\begin{equation}
I=\mathcal{R}^{2}+\mathcal{I}^{2}.
\label{eq15}
\end{equation}

Our goal is to derive the analytical relationship among the complex autocorrelation coefficient $\rho_{S S}=\rho_{S S}^{\Re}+j \rho_{S S}^{\Im}$ of $S$,
the autocorrelation coefficient $\rho_{I I}$ of $I$,
the complex autocorrelation coefficient $\rho_{z z}=\rho_{z z}^{\Re}+j \rho_{z z}^{\Im}$ of $z$, and
the autocorrelation coefficient $\rho_{N N}$ of $N$.
We denote by
$\rho_{S S}^{\Re}, \rho_{S S}^{\Im}$ and $\rho_{z z}^{\Re}, \rho_{z z}^{\Im}$ the real and imaginary components of the complex correlation coefficients $\rho_{S S}$ and $\rho_{z z}$.
Let's denote the autocorrelation coefficient of $x_i$ as $\rho_{x x}$,
the autocorrelation coefficient of $y_i$ as $\rho_{y y}$,
the cross-correlation coefficient of $x_i$ and $y_i$ as $\rho_{y x}$ (or $\rho_{x y}$), then we have~\cite{Jacovitti:1992,Jacovitti:1994,Miller:1974}:
\begin{equation}
\begin{split}
\rho_{z z}^{\Re}(\tau) & = \rho_{x x}(\tau)=\rho_{y y}(\tau)  \\
\rho_{z z}^{\Im}(\tau) & = \rho_{y x}(\tau)=-\rho_{x y}(\tau).
\label{eq16}
\end{split}
\end{equation}
Since $z_i$ is circularly symmetric:
\begin{equation}
\rho_{z z}^{\Im}(\tau) = \rho_{y x}(\tau)=-\rho_{x y}(\tau)=0.
\label{eq17}
\end{equation}

The complex random variable $S$ is a linear combination of multiple circular symmetric random variables $z_i$, therefore $S$ is also a circular symmetric complex Gaussian random variable~\cite{Goodman:2007}. Denote the autocorrelation
coefficient of $\mathcal{R}$ and $\mathcal{I}$ as $\rho_{\mathcal{R} \mathcal{R}}$ and $\rho_{\mathcal{I} \mathcal{I}}$ respectively, the cross-correlation coefficient of $\mathcal{R}$ and $\mathcal{I}$ as $\rho_{\mathcal{I} \mathcal{R}}$ (or $\rho_{\mathcal{R} \mathcal{I}})$, then we have~\cite{Jacovitti:1992,Jacovitti:1994,Miller:1974}:
\begin{equation}
\begin{split}
\rho_{S S}^{\Re}(\tau) & = \rho_{\mathcal{R} \mathcal{R}}(\tau)=\rho_{\mathcal{I} \mathcal{I}}(\tau)  \\
\rho_{S S}^{\Im}(\tau) & = \rho_{\mathcal{I} \mathcal{R}}(\tau)=-\rho_{\mathcal{R} \mathcal{I}}(\tau)= 0,
\label{eq18}
\end{split}
\end{equation}
and
\begin{equation}
\rho_{z z}(\tau) = \rho_{z z}^{\Re}(\tau) , \rho_{S S}(\tau) = \rho_{S S}^{\Re}(\tau).
\label{eq19}
\end{equation}

Therefore, the relationship between $\rho_{z z}(\tau)$ and $\rho_{S S}(\tau)$ is equivalent to the relationship between $\rho_{\mathcal{R} \mathcal{R}}(\tau)$ and $\rho_{x x}(\tau)$ or $\rho_{\mathcal{I} \mathcal{I}}(\tau)$ and $\rho_{y y}(\tau)$.
That is, the problem is to explore the effect of the linear summation process (LSP) on the correlation coefficient.
The relationship between $\rho_{\mathcal{R} \mathcal{R}}(\tau)$ and $\rho_{x x}(\tau)$ are derived as follows. According to Eq.~\eqref{eq14}:
\begin{equation}
\mathcal{R}=\sum_{i=1}^{N} x_{i} \nonumber,
\end{equation}
where the number of scatterers $N$ is a random variable.
The mean and the second moment of $\mathcal{R}$ are:
\begin{equation}
\mathrm{E}\left[\mathcal{R}\right]=\mathrm{E}\left[N\right] \mu_{x}
\label{eq20}
\end{equation}
\begin{equation}
\mathrm{E}\left[\mathcal{R}^{2}\right]=\mathrm{E}\left[N^{2}\right] \mu_{x}^{2}+\sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N} \sum_{j=1}^{N} \rho_{x x}(i, j)\right].
\label{eq21}
\end{equation}
where $\mu_{x}$ and $\sigma_{x}^{2}$ are the mean and variance of $x_i$, respectively; $\rho_{x x}(i,j)$ denotes the correlation coefficient between the $i$-th and the $j$-th scatterers in a resolution cell; $\mathrm{E}_{N}\left[ \cdot \right]$ is the expected value of $N$.

The variance of $\mathcal{R}$ is then:
\begin{equation}
\begin{split}
\sigma_{\mathcal{R}}^{2} & = \mathrm{E}\left[\mathcal{R}^{2}\right]-\mathrm{E}\left[\mathcal{R}\right]
^2   \\
& = \mu_{x}^{2}\left(\mathrm{E}\left[N^{2}\right]-\mathrm{E}[N]^{2}\right)+\sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N} \sum_{j=1}^{N} \rho_{x x}(i, j)\right] \\
& = \mu_{x}^{2}\sigma_{N}^{2}+\sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N} \sum_{j=1}^{N} \rho_{x x}(i, j)\right],
\label{eq22}
\end{split}
\end{equation}
where $\sigma_{N}^{2}$ is the variance $N$.

Define the following random variables:
\begin{align*}
\mathcal{R}(t)&=\sum_{i=1}^{N(t)} x_{i}(t),\\
\mathcal{R}(t-\tau)&=\sum_{i=1}^{N(t-\tau)} x_{i}(t-\tau),
\end{align*}
where $N(t)$ and $N(t-\tau)$ are the number of scatterers at $t$ and $(t-\tau)$, respectively.
The autocorrelation function of $\mathcal{R}$ is:
\begin{equation}
\begin{split}
\mathrm{E}[\mathcal{R}(t) \mathcal{R}(t-\tau)] & = \sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \rho_{x x}(i, j, \tau)\right] \\
& {} + (\rho_{N N}(\tau)\sigma_{N}^{2}+\mu_{N}^{2})\mu_{x}^{2},
\label{eq23}
\end{split}
\end{equation}
where $\rho_{x x}(i, j, \tau)$ is the correlation coefficient between the $i$-th scatterer in one resolution cell and the $j$-th scatterer in the other resolution cell in a distance $\tau$, and $\sigma_{N}^{2}$ and $\mu_N$ are the variance and mean of $N$, respectively.
The detailed derivation is given in the Appendix.

The autocorrelation coefficient $\rho_{\mathcal{R} \mathcal{R}}(\tau)$ of $\mathcal{R}$ is:
\begin{align}
& \rho_{\mathcal{R} \mathcal{R}}(\tau) = {}\nonumber\\
& \frac{\mathrm{E}[\mathcal{R}(t) \mathcal{R}(t-\tau)]-(\mathrm{E}[\mathcal{R}])^{2}}{\sigma_{\mathcal{R}}
^{2}} = {}\nonumber\\
& \frac{\rho_{N N}(\tau) \sigma_{N}^{2} \mu_{x}^{2}+\sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \rho_{x x}(i, j, \tau)\right]}{\mu_{x}^{2} \sigma_{N}^{2}+\sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N} \sum_{j=1}^{N} \rho_{x x}(i, j)\right]}.\label{eq24}
\end{align}
Eq.~\eqref{eq24} can be simplified making the following assumptions.

\subsubsection{Assumption 1: different scatterers are independent}
Different scatterer types within one resolution cell are mutually independent, that is \cite{Allan:2006,Collins:2009}
\begin{equation}
\rho_{x x}(i,j) = \begin{cases}
1, & \text{if } i = j; \\
0, & \text{if } i \neq j.
\label{eq25}
\end{cases}
\end{equation}
then we have:
\begin{equation}
\mathrm{E}_{N}\left[\sum_{i=1}^{N} \sum_{j=1}^{N} \rho_{x x}(i, j)\right] = \mu_{N}.
\label{eq26}
\end{equation}

Further assuming that the different scatterer types (i.e. $i \neq j$) are mutually independent in general, that is
\begin{equation}
\rho_{x x}(i, j, \tau) = \begin{cases}
\rho_{x x}(i, \tau), & \text{if } i = j; \\
0, & \text{if } i \neq j.
\label{eq27}
\end{cases}
\end{equation}
then we have:
\begin{equation}
\begin{split}
& \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \rho_{x x}(i, j, \tau)\right] \\
& = \mathrm{E}_{N}\left[\sum_{i=1}^{\min \{N(t), N(t-\tau)\}} \rho_{x x}(i, \tau)\right] \\
& = \mathrm{E}[\min \{N(t), N(t-\tau)\}] \mathrm{E}\left[\rho_{x x}(i, \tau)\right].
\label{eq28}
\end{split}
\end{equation}
Substituting~\eqref{eq26} and~\eqref{eq28} in~\eqref{eq24}, we have:
\begin{align}
\rho_{\mathcal{R} \mathcal{R}}(\tau) & = \frac{\rho_{N N}(\tau) \sigma_{N}^{2} \mu_{x}^{2}}{\mu_{x}^{2} \sigma_{N}^{2}+\sigma_{x}^{2} \mu_{N}} \nonumber\\
& + \frac{\sigma_{x}^{2} \mathrm{E}[\min \{N(t), N(t-\tau)\}] \mathrm{E}\left[\rho_{x x}(i, \tau)\right]}{\mu_{x}^{2} \sigma_{N}^{2}+\sigma_{x}^{2} \mu_{N}}.
\label{eq29}
\end{align}

\subsubsection{Assumption 2: zero-mean independent random variables}

Assuming that $x_i$ and $y_i$ are zero-mean independent identically distributed (Gaussian) random variables, i.e.,
$\mu_x  = \mu_y = 0$, $\sigma_{x}^{2} = \sigma_{y}^{2}$,
$\rho_{x y}(\tau) = 0$, then Eq.~\eqref{eq29} can be further simplified to:
\begin{equation}
\rho_{\mathcal{R} \mathcal{R}}(\tau)=\frac{\mathrm{E}[\min \{N(t), N(t-\tau)\}]}{\mu_{N}} \mathrm{E}\left[\rho_{x x}(i, \tau)\right].
\label{eq30}
\end{equation}
This equation establishes the relationship between $\rho_{x x}(i, \tau)$ and $\rho_{\mathcal{R} \mathcal{R}}(\tau)$.
The autocorrelation coefficient $\rho_{\mathcal{R} \mathcal{R}}(\tau)$ of the scattered field is determined by the autocorrelation coefficient $\rho_{x x}(i, \tau)$ of the underlying Gaussian scatterer and by the spatial fluctuation of the number of scatterers.
The correlation length $\ell_\mathcal{R}$ of the real-component image is totally determined by the correlation length $\ell_x$ of the underlying Gaussian scatterer.
Let’s define the correlation factor of the number of scatterers $c_N$ as:
\begin{equation}
c_{N}(\tau) = \frac{\mathrm{E}[\min \{N(t), N(t-\tau)\}]}{\mu_{N}}.
\label{eq31}
\end{equation}
Note that $\mathrm{E}[\min \{N(t), N(t-\tau)\}]$ is very difficult to derive explicitly, and thus it is calculated numerically.
In particular, if $N$ is constant, then it has $c_{N}(\tau) = 1$.
Furthermore, if the correlation length $\ell_N$ of the number of scatterers is much larger than the correlation length $\ell_x$ of the underlying Gaussian scatterer, i.e., if $\ell_{N} \gg \ell_x$, and the autocorrelation coefficient $\rho_{N N}(\tau)$ satisfies:
\begin{equation}
\rho_{N N}(\tau) \rightarrow 1, \text{if } \tau \leq \ell_{x},
\label{eq32}
\end{equation}
then:
\begin{equation}
N(t) \approx N(t-\tau), \text{if } \tau \leq \ell_{x},
\label{eq33}
\end{equation}
\begin{align}
& \mathrm{E}[\min \{N(t), N(t-\tau)\}] \nonumber \\
={} & \int \min \{N(t), N(t-\tau)\} p(N(t), N(t-\tau)) d t  \nonumber \\
={} & \int \min \{N(t), N(t-\tau)\} p(N(t)\mid N(t-\tau)) p(N(t)) d t \nonumber \\\
\approx {} & \int N(t) p(N(t)) d t
= \mu_{N},
\label{eq34}
\end{align}
then
\begin{equation}
c_{N}(\tau) \approx \frac{\mu_{N}}{\mu_{N}} = 1.
\label{eq35}
\end{equation}
Therefore, the influence of the distribution of the number of scatterers on the correlation characteristics is negligible.
It gives the theoretical relationship among the autocorrelation coefficient $\rho_{S S}$ of the complex SAR image, the autocorrelation coefficient $\rho_{z z}$ of the underlying Gaussian scatterer and the fluctuation of the scatterer number $N$.

We derive in the following the autocorrelation coefficient $\rho_{I I}$, the scattered intensity $I$.
Details are in the Appendix.
With the above-mentioned assumptions, the mean intensity $\mu_{I}$ is:
\begin{equation}
\mu_{I} = \mu_{N}(\sigma_{x}^{2}+\sigma_{y}^{2}) = 2 \mu_{N} \sigma_{x}^{2}.
\label{eq36}
\end{equation}
The variance of $I$ is then:
\begin{equation}
\sigma_{I}^{2}=\mathrm{E}\left[I^{2}\right]-\mu_{I}^{2}=\left(8 \sigma_{N}^{2}+4 \mu_{N}^{2}\right) \sigma_{x}^{4}.
\label{eq37}
\end{equation}
The autocorrelation coefficient $\rho_{I I}(\tau)$ of $I$ is given by:
\begin{align}
\rho_{I I}(\tau) & =\frac{\mathrm{E}[I(0) I(\tau)]-\mu_{I}^{2}}{\sigma_{I}^{2}} \nonumber\\
& = \frac{\mathrm{E}\left[N_{m} N_{m}\right] \rho_{x x}^{2}(\tau)+\rho_{N N}(\tau) \sigma_{N}^{2}}{2 \sigma_{N}^{2}+\mu_{N}^{2}}.
\label{eq38}
\end{align}
where $N_{0} = N(0)$, $N_{\tau} = N(\tau)$, $N_{m} = \min \left\{N_{0}, N_{\tau}\right\}$ as shown in the Appendix.

Eq.~\eqref{eq38} establishes the theoretical relationship among
the autocorrelation coefficient of the intensity image $\rho_{I I}(\tau)$,
the autocorrelation coefficient $\rho_{x x}(\tau)$ of the real-component image,
and the autocorrelation coefficient $\rho_{N N}(\tau)$ of the number of scatterers.
Differently from Eq.~\eqref{eq30},
the correlation length $\ell_{I}$ of $\rho_{I I}(\tau)$ is determined by both $\ell_x$ and $\ell_N$.
Both the texture feature of the underlying Gaussian scatterer and of the number of scatterers contribute together to the texture of the intensity image.
Usually, the correlation length $\ell_x$ of the underlying Gaussian scatterer is much smaller than $\ell_N$, which implies that the large-scale texture in the intensity image is mainly governed by the texture $\rho_{N N}(\tau)$ of the number of scatterers, while the small-scale texture information depends on both $\rho_{x x}(\tau)$ and $\rho_{N N}(\tau)$.
This finding provides us a viable way to model the texture information of SAR images in a realistic manner.

Similarly to Eq.~\eqref{eq31}, we now define a correlation factor of the number of scatterers:
\begin{align}
c_{N^2}(\tau) = \frac{\mathrm{E}\left[N_{m} N_{m}\right]}{\mathrm{E}\left(N^{2}\right)}
=\frac{\mathrm{E}\left[\min \left\{N_{0}, N_{\tau}\right\} \min \left\{N_{0}, N_{\tau}\right\}\right]}{\sigma_{N}^{2}+\mu_{N}^{2}}.
\label{eq39}
\end{align}
If $\rho_{N N}(\tau) \rightarrow 1$, then $c_{N^{2}}(\tau) \rightarrow 1$. If $\ell_{N} \gg \ell_x$, and Eq.~\eqref{eq32} is satisfied, then Eq.~\eqref{eq38} can be approximated by:
\begin{align}
\rho_{I I}(\tau) & = \frac{c_{N^{2}}(\tau) \mathrm{E}\left[N^{2}\right] \rho_{x x}^{2}(\tau)+\rho_{N N}(\tau) \sigma_{N}^{2}}{2 \sigma_{N}^{2}+\mu_{N}^{2}} \nonumber \\
& \approx \begin{cases}
1,& \text{if } \tau=0 ;\\
\frac{\left(\sigma_{N}^{2}+\mu_{N}^{2}\right) \rho_{x x}^{2}(\tau)+\rho_{N N}(\tau) \sigma_{N}^{2}}{2 \sigma_{N}^{2}+\mu_{N}^{2}},& \text{if } 0<\tau \leq \ell_{x} ; \\
\frac{\rho_{N N}(\tau) \sigma_{N}^{2}}{2 \sigma_{N}^{2}+\mu_{N}^{2}},& \text{if } \tau > \ell_{x}.
\label{eq40}
\end{cases}.
\end{align}

Given $\rho_{I I}(\tau)$ and $\rho_{x x}(\tau)$, $\rho_{N N}(\tau)$ can be calculated by the inversion of Eq.~\eqref{eq40}:
\begin{equation}
\rho_{N N}(\tau) \approx \begin{cases}
1,& \text{if } \tau=0 ;\\
\frac{\rho_{I I}(\tau)\left(2 \sigma_{N}^{2}+\mu_{N}^{2}\right)-\left(\sigma_{N}^{2}+\mu_{N}^{2}\right) \rho_{x x}^{2}(\tau)}{\sigma_{N}^{2}},& \text{if } 0<\tau \leq \ell_{x} ;\\
\frac{\rho_{I I}(\tau)\left(2 \sigma_{N}^{2}+\mu_{N}^{2}\right)}{\sigma_{N}^{2}},& \text{if } \tau > \ell_{x}.
\label{eq41}
\end{cases}
\end{equation}

The following inequalities must be satisfied to ensure that $\rho_{N N}(\tau) \in[-1,1]$:
\begin{equation}
\left\{\begin{aligned} \frac{\rho_{I I}(\tau)-\rho_{x x}^{2}(\tau)}{1-\rho_{I I}(\tau)} \leq \frac{\sigma_{N}^{2}}{\sigma_{N}^{2}+\mu_{N}^{2}},\quad & \text{if } 0<\tau \leq \ell_{x} ; \\ \frac{\rho_{I I}(\tau)-\rho_{x x}^{2}(\tau)}{1+\rho_{I I}(\tau)} \geq -\frac{\sigma_{N}^{2}}{\sigma_{N}^{2}+\mu_{N}^{2}},\quad & \text{if } 0<\tau \leq \ell_{x} ; \\
\left|\rho_{I I}(\tau)\right|\leq \frac{\sigma_{N}^{2}}{2 \sigma_{N}^{2}+\mu_{N}^{2}},\quad & \text{if } \tau > \ell_{x}. \end{aligned}\right.
\label{eq42}
\end{equation}

\section{Correlated SAR Texture Simulation Using the Generalized Model}\label{Sec:Simulation}

In this section, we will verify the ability of the generalized model to represent correlated SAR texture using both simulated data and actual SAR images.

Without loss of generality, we employed a Gaussian convolution kernel $h(n_1,n_2)$ with standard deviation $\sigma = 1.5$ for the underlying Gaussian scatterers:
\begin{equation}
h\left(n_{1}, n_{2}\right)=\frac{\exp\bigl\{-\frac{(n_{1}^{2}+n_{2}^{2})}{2 \sigma^{2}}\bigr\}}{\sum_{n_{1}} \sum_{n_{2}} \exp\bigl\{-\frac{(n_{1}^{2}+n_{2}^{2})}{2 \sigma^{2}}\bigr\}},
\label{eq43}
\end{equation}
where $n_1,n_2\in\{-5,\dots,5\}$.

Fig.~\ref{Figure6} shows the simulation results of correlated clutter following
Rayleigh,
S$\upalpha$SGR,
$K$,
$G^0$,
$U$,
and $W$ distributions using the same Gaussian kernel.
It shows different SAR texture patterns of size $100\times100$ pixels.
Notice that they differ greatly due to the difference in the single-point (marginal) distribution of the data.

\begin{figure*}[hbt]
\centering
% Requires \usepackage{graphicx}
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure6-(a)Rayleigh_h}
	\caption{Rayleigh distribution \\($\sigma$ = 1)}\label{Figure6(a)}
\end{subfigure}
\qquad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure6-(b)SaSGR_h}
	\caption{S$\upalpha$GR distribution \\($\alpha$ = 1.8, $\gamma$ = 10)}\label{Figure6(b)}
\end{subfigure}
\qquad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure6-(c)K_h}
	\caption{K distribution \\($\alpha$ = 10, $\beta$ = 3)}\label{Figure6(c)}
\end{subfigure}
\qquad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure6-(d)G0_h}
	\caption{$G^0$ distribution \\($\alpha$ = 20, $\gamma$ = 2)}\label{Figure6(d)}
\end{subfigure}
\qquad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure6-(e)U_h}
	\caption{$U$ distribution \\($p$ = 5, $q$ = 8, $\beta$ = 50)}\label{Figure6(e)}
\end{subfigure}
\qquad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure6-(f)W_h}
	\caption{$W$ distribution \\($p$ = 5, $q$ = 10, $\beta$ = 50)}\label{Figure6(f)}
\end{subfigure}
\caption{Simulated correlated SAR images using the Gaussian kernel in Eq. \eqref{eq43} with the generalized model}\label{Figure6}
\end{figure*}

Fig.~\ref{Figure7} summarizes a procedure to simulate textured SAR data using the generalized model.

\begin{figure}[hbt]
\centering
% Requires \usepackage{graphicx}
%\includegraphics[width=\linewidth]{Figure7-Flowchart}
\caption{Flowchart of correlated SAR clutter simulation by the generalized model}\label{Figure7}
\end{figure}

The input is an image sample, with which one infers the single-point distribution, its parameters, and the correlation structure.
%
With this information, and using Table~\ref{Table1}, one obtains the parameters of the distributions of the underlying Gaussian scatterers and of the number of scatterers for the generalized model.
%
The correlation coefficients of the underlying Gaussian scatterers and number of scatterers are also determined by the correlation coefficient of the real-component and intensity SAR data according to the correlation relationship of Eqs.~\eqref{eq30} and~\eqref{eq38}.
%%% ACF Please if the two references above are correct; it is not clear how they relate the sample input with the simulation parameters
%%% Yue Thanks for your reminding. The two references of equations above are wrong because of the changed labels. I have corrected it, and then they can relate the sample input with the simulation parameters.
%
And then the corresponding convolution kernels are obtained according to Eq.~\eqref{eq11} which can be implemented by Fourier transform on both sides of the equation.

Finally, given
(i)~the parameters of the underlying Gaussian scatterers,
(ii)~the parameters of the number of scatterers, and
(iii)~the convolution kernels,
the generalized model can be used to obtain data with same marginal distribution and correlation structure as to the input sample.

The marginal distribution must first be selected.
Model selection can be done by goodness-of-fit tests, among them:
the $\chi^{2}$ test~\cite{Papoulis:2002,Devore:2004},
the Kolmogorov-Smirnov test~\cite{Devore:2004},
the Kullback-Leibler distance~\cite{Kullback:1959}, and
the D'Agostino-Pearson test~\cite{Devore:2004}.
Parameter estimation methods include:
Method of Moments (MoM)~\cite{Papoulis:2002},
Maximum Likelihood (ML)~\cite{Papoulis:2002}, and
Method of LogCumulants (MoLC)~\cite{Nicolas:2002}.

In the following experiments, the distribution type is chosen empirically according to the terrain types, while the parameters are estimated by the MoM method.

Fig.~\ref{Figure8-SelectedPatches} shows the samples we used.
They are from images acquired by the L-band NASA/JPL UAVSAR, C-band GaoFen-3 (GF3), L-band ALOS-2, and X-band TerraSAR-X.
We selected terrain types including
sea areas with different wind waves,
an urban area,
forests with varying topography,
farmland, and bare land.

\begin{figure*}[hbt]
\centering
% Requires \usepackage{graphicx}
%\includegraphics[width=15 cm]{Figure8-SelectedPatches}
\caption{Texture patches from SAR images}\label{Figure8-SelectedPatches}
\end{figure*}

Rayleigh distribution, the first representation of the K distribution and the second representation of the RiIG distribution are empirically selected to model SAR samples.

The parameters of the Gaussian distribution and of the number of scatterers, as well as the convolution kernels, are estimated and then used to simulate correlated SAR images as shown in Figs.~\ref{Figure9}, \ref{Figure10}, \ref{Figure11}, and~\ref{Figure12}.

Figs.~\ref{Figure9}, \ref{Figure10}, \ref{Figure11}, and~\ref{Figure12} compare actual SAR samples with the simulated ones.
The method performs well on the most sea and vegetated clutters (farmlands and forests).
However, it fails to capture the deterministic patterns such as urban or mountain areas, which is expected as the proposed model itself is only for stationary stochastic fields.
This point is further illustrated in detail in Figs.~\ref{Figure11} and~\ref{Figure12}.

\begin{figure*}[hbt]
\centering
% Requires \usepackage{graphicx}
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea1Real}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea2Real}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea3Real}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea4Real}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea1Simulated}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea2Simulated}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea3Simulated}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure9-Sea4Simulated}
\end{subfigure}
\caption{Comparison of real (top row) and simulated (bottom row) SAR textures of various scenarios acquired by different systems, from left to right: Sea1, Sea2, Sea3 and Sea4 as labeled in Fig.~\ref{Figure8-SelectedPatches}}\label{Figure9}
\end{figure*}

\begin{figure*}[hbt]
\centering
% Requires \usepackage{graphicx}
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-UrbanReal}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-FarmlandReal}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-Forest2Real}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-RoadReal}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-UrbanSimulated}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-FarmlandSimulated}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-Forest2Simulated}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure10-RoadSimulated}
\end{subfigure}
\caption{Comparison of real (top row) and simulated (bottom row) SAR textures of various scenarios acquired by different systems, from left to right: Urban, Farmland, Forest2 and Road as labeled in Fig.~\ref{Figure8-SelectedPatches}}\label{Figure10}
\end{figure*}

Figs.~\ref{Figure11} and~\ref{Figure12} show comparisons between samples from forest and mountains and their respective simulated data.
The first row shows the samples,
the second the histogram and fitted densities of amplitude and phase,
and the third the amplitude correlation.
It can be seen that for both cases, i.e. Forest2 in Fig.~\ref{Figure11} and Mountain in Fig.~\ref{Figure12}, the simulated PDFs and correlation functions match well with the measured ones.
The final simulated SAR texture of forest looks much closer to the sample than that for the mountain scenario.
This is evidence that the method is working but the model is not capable of representing deterministic patterns as shown in the second case.
For such situations, one may resort to deterministic approaches, or to larger convolution masks.

\begin{figure}[hbt]
\centering
% Requires \usepackage{graphicx}
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure11-Forest1(a)}
	\caption{Real data}\label{Figure11(a)}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure11-Forest1(b)}
	\caption{Simulation}\label{Figure11(b)}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure11-Forest1(c)}
	\caption{Amplitude distribution}\label{Figure11(c)}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure11-Forest1(d)}
	\caption{Phase distribution}\label{Figure11(d)}
\end{subfigure}
\quad
\begin{subfigure}[t]{8 cm}
	\centering
%	\includegraphics[width=8 cm]{Figure11-Forest1(e)}
	\caption{Amplitude correlation}\label{Figure11(e)}
\end{subfigure}
\caption{Simulation results of Forest1 as labeled in Fig.~\ref{Figure8-SelectedPatches}}\label{Figure11}
\end{figure}

\begin{figure}[hbt]
\centering
% Requires \usepackage{graphicx}
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure12-Mountain(a)}
	\caption{Real data}\label{Figure12(a)}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure12-Mountain(b)}
	\caption{Simulation}\label{Figure12(b)}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure12-Mountain(c)}
	\caption{Amplitude distribution}\label{Figure12(c)}
\end{subfigure}
\quad
\begin{subfigure}[t]{4 cm}
	\centering
%	\includegraphics[width=4 cm]{Figure12-Mountain(d)}
	\caption{Phase distribution}\label{Figure12(d)}
\end{subfigure}
\quad
\begin{subfigure}[t]{8 cm}
	\centering
%	\includegraphics[width=8 cm]{Figure12-Mountain(e)}
	\caption{Amplitude correlation}\label{Figure12(e)}
\end{subfigure}
\caption{Simulation results of Mountain as labeled in Fig. \ref{Figure8-SelectedPatches}}\label{Figure12}
\end{figure}

\section{Conclusions}\label{Sec:Conclusions}

We proposed a generalized framework for correlated SAR texture modeling based on a Gaussian coherent scatterer model.
It is physics-plausible in nature because it is derived from the coherent scattering process of the simplest Gaussian distributed scatterer.

On the one hand, the proposed model is able to represent a large variety of single-point probability distribution models which are commonly used in the literature and practice of SAR image processing and analysis.
We show that the proposed model includes the eight most important existing single-point probability distributions for amplitude data, namely
Rayleigh,
heavy-tailed Generalized Rayleigh distribution,
$K$,
$G^0$,
$W$,
$U$,
Rice,
Rician Inverse Gaussian, and
Symmetric $\upalpha$-stable.

On the other hand, the proposed model can easily describe the texture information of SAR images by introducing spatially correlation structures in the underlying Gaussian scatterers and in the number of scatterers.
We derive the expressions of the correlation coefficient for texture modeling of the generalized model.

We propose a method for generating plausible SAR textures based on this model, and we show its expressiveness and tractability by extracting samples of various scenarios from actual images and then simulating them.
We also identify some limitations of the approach.

The proposed simulation framework may be used to improve the understanding of SAR images.
Simulated patched may serve for the training of neural networks, and also can be used for classification and filter assessment within Monte Carlo studies.

Code and data are available at https://github.com/fudanxu/SAR-GGCS.

%\appendices
\section{}

This appendix gives the detailed derivation of the autocorrelation coefficient $\rho_{\mathcal{R} \mathcal{R}}(\tau)$ in~\eqref{eq24}, and $\rho_{I I}(\tau)$ in~\eqref{eq38}.
\setcounter{equation}{0}
\renewcommand\theequation{A.\arabic{equation}}

Denote $\mathcal{R}(t)=\sum_{i=1}^{N(t)} x_{i}(t)$ and $\mathcal{R}(t-\tau)=\sum_{i=1}^{N(t-\tau)} x_{i}(t-\tau)$,
random variables at distance $\tau$, where $N(t)$ and $N(t-\tau)$ are the number of scatterers at $t$ and $(t-\tau)$, respectively.

The autocorrelation function of $\mathcal{R}$ is derived as:
\begin{align}
& \mathrm{E}[\mathcal{R}(t) \mathcal{R}(t-\tau)] \nonumber\\
= {} & \mathrm{E}\left[\sum_{i=1}^{N(t)=n_{1}} x_{i}(t) \sum_{i=1}^{N(t-\tau)=n_{2}} x_{i}(t-\tau)\right] \nonumber\\
= {} & \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \mathrm{E}\left[x_{i}(t) x_{j}(t-\tau)\right]\right] \nonumber\\
= {} & \mathrm{E}_{N}\left[\sum_{i=1}^{N(t) N(t-\tau)}\left(\rho_{x x}(i, j, \tau) \sigma_{x}^{2}+\mu_{x}^{2}\right)\right] \nonumber\\
= {} & \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \rho_{x x}(i, j, \tau) \sigma_{x}^{2}\right] + \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \mu_{x}^{2}\right] \nonumber\\
= {} & \sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \rho_{x x}(i, j, \tau)\right] + \mathrm{E}[N(t) N(t-\tau)] \mu_{x}^{2} \nonumber\\
= {} & \sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \rho_{x x}(i, j, \tau)\right] + \left(\rho_{N N}(\tau) \sigma_{N}^{2}+\mu_{N}^{2}\right) \mu_{x}^{2},
\label{eqA2}
\end{align}
where $\rho_{x x}(i, j, \tau)$ is the correlation coefficient between the $i$-th scatterer in one resolution cell and the $j$-th scatterer in the other resolution cell in a distance $\tau$.
The mean and variance of $N$ are, respectively, $\mu_{N}$ and
$\sigma_{N}^{2}$.

The autocorrelation coefficient $\rho_{\mathcal{R} \mathcal{R}}(\tau)$ of $\mathcal{R}$ is derived as:
\begin{align}
\rho_{\mathcal{R} \mathcal{R}}(\tau) & = \frac{\mathrm{E}[\mathcal{R}(t) \mathcal{R}(t-\tau)]-(\mathrm{E}[\mathcal{R}])^{2}}{\sigma_{\mathcal{R}}^{2}} \nonumber\\
& = \frac{\rho_{N N}(\tau) \sigma_{N}^{2} \mu_{x}^{2}+\sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t-\tau)} \rho_{x x}(i, j, \tau)\right]}{\mu_{x}^{2} \sigma_{N}^{2}+\sigma_{x}^{2} \mathrm{E}_{N}\left[\sum_{i=1}^{N} \sum_{j=1}^{N} \rho_{x x}(i, j)\right]}.
\label{eqA3}
\end{align}

Eq.~\eqref{eq15} defined the scattered intensity as $I = \mathcal{R}^{2} + \mathcal{I}^{2}$.
With the two assumptions above, the mean $\mu_I$ of the intensity is:
\begin{align}
\mu_{I} & = \mathrm{E}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t)}\left(x_{i} x_{j}+y_{i} y_{j}\right)\right] \nonumber\\
& = \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t)} \mathrm{E}\left[x_{i} x_{j}+y_{i} y_{j}\right]\right] \nonumber\\
& = \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t)}\left(\rho_{x x}(i, j) \sigma_{x}^{2}+\rho_{y y}(i, j) \sigma_{y}^{2}\right)\right] \nonumber\\
& = \mathrm{E}_{N}\left[\sum_{i=1}^{N(t)}\left(\sigma_{x}^{2}+
\sigma_{y}^{2}\right)\right] = \mu_{N}\left(\sigma_{x}^{2}+\sigma_{y}^{2}\right)
= 2 \mu_{N} \sigma_{x}^{2}.
\label{eqA5}
\end{align}

The second moment of $I$ is:
\begin{align}
\mathrm{E}\left[I^{2}\right] & = \mathrm{E}\left[\sum_{i=1}^{N(t)} \sum_{j=1}^{N(t)}\left(x_{i} x_{j}+y_{i} y_{j}\right) \sum_{k=1}^{N(t) }\sum_{\ell=1}^{N(t)}\left(x_{k} x_{\ell} + y_{k} y_{\ell}\right)\right] \nonumber\\
& = \sum_{i=1}^{N(t)} \sum_{j=1}^{N(t)} \sum_{k=1}^{N(t)} \sum_{\ell=1}^{N(t)}
\mathrm{E}\left[x_{i} x_{j} x_{k} x_{\ell}+x_{i} x_{j} y_{k} y_{\ell}\right] \nonumber\\
& \qquad \qquad \qquad \qquad + \mathrm{E}\left[y_{i} y_{j} x_{k} x_{\ell}+y_{i} y_{j} y_{k} y_{\ell}\right].
\label{eqA6}
\end{align}
And, with the assumptions above, it becomes:
\begin{equation}
\mathrm{E}\left[x_{i} x_{j} x_{k} x_{\ell}\right] = \begin{cases}
\mathrm{E}\left[x_{i}^{4}\right], & \text{if } i=j=k=\ell; \\
\mathrm{E}\left[x_{i}^{2}\right] \mathrm{E}\left[x_{k}^{2}\right], & \text{if } i=j \neq k=\ell; \\
\mathrm{E}\left[x_{i}^{2}\right] \mathrm{E}\left[x_{j}^{2}\right], &  \text{if } i=k \neq j=\ell; \\
\mathrm{E}\left[x_{i}^{2}\right] \mathrm{E}\left[x_{j}^{2}\right], &  \text{if } i=\ell \neq j=k; \\
0, & \text{if } \text {else}.
\label{eqA7}
\end{cases}
\end{equation}

\begin{equation}
\mathrm{E}\left[y_{i} y_{j} y_{k} y_{\ell}\right] = \begin{cases}
\mathrm{E}\left[y_{i}^{4}\right], & \text{if } i=j=k=\ell; \\
\mathrm{E}\left[y_{i}^{2}\right] \mathrm{E}\left[y_{k}^{2}\right], & \text{if } i=j \neq k=\ell; \\
\mathrm{E}\left[y_{i}^{2}\right] \mathrm{E}\left[y_{j}^{2}\right], &  \text{if } i=k \neq j=\ell; \\
\mathrm{E}\left[y_{i}^{2}\right] \mathrm{E}\left[y_{j}^{2}\right], &  \text{if } i=\ell \neq j=k; \\
0, & \text{if } \text {else}.
\label{eqA8}
\end{cases}
\end{equation}

\begin{align}
\mathrm{E}\left[x_{i} x_{j} y_{k} y_{\ell}\right] & = \mathrm{E}\left[y_{i} y_{j} x_{k} x_{\ell}\right] \nonumber\\
& = \begin{cases}
\mathrm{E}\left[x_{i}^{2}\right] \mathrm{E}\left[y_{i}^{2}\right], & \text{if } i=j=k=\ell; \\
\mathrm{E}\left[y_{i}^{2}\right] \mathrm{E}\left[x_{k}^{2}\right], & \text{if } i=j \neq k=\ell; \\
\mathrm{E}\left[x_{i}y_{i}\right] \mathrm{E}\left[x_{j}y_{j}\right] = 0, &  \text{if } i=k \neq j=\ell; \\
\mathrm{E}\left[x_{i}y_{i}\right] \mathrm{E}\left[x_{j}y_{j}\right] = 0, &  \text{if } i=\ell \neq j=k; \\
0, & \text{if } \text {else}.
\label{eqA9}
\end{cases}
\end{align}

Replacing~\eqref{eqA7}, \eqref{eqA8} and~\eqref{eqA9} in~\eqref{eqA6}, we obtain:
\begin{equation}
\begin{split}
\mathrm{E}\left[ I^2 \right] = & \mathrm{E}\left[N\right]\mathrm{E}\left[x_{i}^{4}\right]+3 \left\{\mathrm{E}\left[ N^{2}\right]-\mathrm{E}\left[N\right]\right.\}\mathrm{E}\left[x_{i}^{2}\right] \mathrm{E}\left[x_{i}^{2}\right] \\
& + \mathrm{E}\left[N\right]\mathrm{E}\left[y_{i}^{4}\right]+3 \left\{\mathrm{E}\left[ N^{2}\right]-\mathrm{E}\left[N\right]\right.\}\mathrm{E}\left[y_{i}^{2}\right] \mathrm{E}\left[y_{i}^{2}\right] \\
& + 2 \mathrm{E}\left[N\right]\mathrm{E}\left[x_{i}^{2}\right] \mathrm{E}\left[y_{i}^{2}\right] \\
& + 2 \left\{\mathrm{E}\left[ N^{2}\right]-\mathrm{E}\left[N\right]\right.\}\mathrm{E}\left[x_{i}^{2}\right] \mathrm{E}\left[y_{i}^{2}\right].
\label{eqA10}
\end{split}
\end{equation}
Now, using that~\cite{Papoulis:2002}:
\begin{equation}
\begin{split}
& \mathrm{E}\left[x_{i}^{2}\right] = \mathrm{E}\left[y_{i}^{2}\right] = \sigma_{x}^{2} \\
& \mathrm{E}\left[x_{i}^{4}\right] = \mathrm{E}\left[y_{i}^{4}\right] = 3 \sigma_{x}^{4} ,\\
\label{eqA11}
\end{split}
\end{equation}
then~\eqref{eqA10} reduces to:
\begin{equation}
\mathrm{E}\left[I^{2}\right] = 8 \mathrm{E}\left[N^{2}\right]\sigma_{x}^{4} = 8(\sigma_{N}^{2} + \mu_{N}^{2})\sigma_{x}^{4}.
\label{eqA12}
\end{equation}

The variance of $I$ is then derived as:
\begin{equation}
\sigma_{I}^{2} = \mathrm{E}\left[I^{2}\right] - \mu_{I}^{2} = (8\sigma_{N}^{2} + 4\mu_{N}^{2})\sigma_{x}^{4}.
\label{eqA13}
\end{equation}

The autocorrelation function of $I$ is derived as:
\begin{align}
\mathrm{E}\left[I(0) I(\tau)\right] & = \mathrm{E}\left[\sum_{i=1}^{N(0)} \sum_{j=1}^{N(0)}\left(x_{i} x_{j}+y_{i} y_{j}\right) \sum_{k=1}^{N(\tau) }\sum_{\ell=1}^{N(\tau)}\left(x_{k} x_{\ell} + y_{k} y_{\ell}\right)\right] \nonumber\\
& = \sum_{i=1}^{N(0)} \sum_{j=1}^{N(0)} \sum_{k=1}^{N(\tau)} \sum_{\ell=1}^{N(\tau)}
\mathrm{E}\left[x_{i}(0) x_{j}(0) x_{k}(\tau) x_{\ell}(\tau)\right] \nonumber \\
& \qquad \qquad \qquad \qquad + \mathrm{E}\left[x_{i}(0) x_{j}(0) y_{k}(\tau) y_{\ell}(\tau)\right] \nonumber\\
& \qquad \qquad \qquad \qquad + \mathrm{E}\left[y_{i}(0) y_{j}(0) x_{k}(\tau) x_{\ell}(\tau)\right] \nonumber\\
& \qquad \qquad \qquad \qquad + \mathrm{E}\left[y_{i}(0) y_{j}(0) y_{k}(\tau) y_{\ell}(\tau)\right],
\label{eqA14}
\end{align}
where
\begin{align}
& \mathrm{E}\left[x_{i}(0) x_{j}(0) x_{k}(\tau) x_{\ell}(\tau)\right] \nonumber \\
={} & \begin{cases}
\mathrm{E}\left[x_{i}^{2}(0) x_{i}^{2}(\tau)\right], & \text{if } i=j=k=\ell; \\
\mathrm{E}\left[x_{i}^{2}(0)\right] \mathrm{E}\left[x_{k}^{2}(\tau)\right], & \text{if } i=j \neq k=\ell; \\
\mathrm{E}\left[x_{i}(0) x_{i}(\tau)\right] \mathrm{E}\left[x_{j}(0) x_{j}(\tau)\right] = 0, &  \text{if } i=k \neq j=\ell; \\
\mathrm{E}\left[x_{i}(0) x_{i}(\tau)\right] \mathrm{E}\left[x_{j}(0) x_{j}(\tau)\right] = 0, &  \text{if } i=\ell \neq j=k; \\
0, & \text {else.}
\end{cases}\label{eqA15}
\end{align}

\begin{align}
& \mathrm{E}\left[y_{i}(0) y_{j}(0) y_{k}(\tau) y_{\ell}(\tau)\right] \nonumber \\
={} & \begin{cases}
\mathrm{E}\left[y_{i}^{2}(0) y_{i}^{2}(\tau)\right], & \text{if } i=j=k=\ell; \\
\mathrm{E}\left[y_{i}^{2}(0)\right] \mathrm{E}\left[y_{k}^{2}(\tau)\right], & \text{if } i=j \neq k=\ell; \\
\mathrm{E}\left[y_{i}(0) y_{i}(\tau)\right] \mathrm{E}\left[y_{j}(0) y_{j}(\tau)\right] = 0, &  \text{if } i=k \neq j=\ell; \\
\mathrm{E}\left[y_{i}(0) y_{i}(\tau)\right] \mathrm{E}\left[y_{j}(0) y_{j}(\tau)\right] = 0, &  \text{if } i=\ell \neq j=k; \\
0, & \text {else.}
\end{cases}\label{eqA16}
\end{align}

\begin{align}
& \mathrm{E}\left[x_{i}(0) x_{j}(0) y_{k}(\tau) y_{\ell}(\tau)\right] \nonumber \\
={} & \mathrm{E}\left[y_{i}(0) y_{j}(0) x_{k}(\tau) x_{\ell}(\tau)\right] \nonumber \\
={} & \begin{cases}
\mathrm{E}\left[x_{i}^{2}(0)\right]\mathrm{E}\left[y_{i}^{2}(\tau)\right], & \text{if } i=j=k=\ell; \\
\mathrm{E}\left[y_{i}^{2}(0)\right] \mathrm{E}\left[x_{k}^{2}(\tau)\right], & \text{if } i=j \neq k=\ell; \\
\mathrm{E}\left[x_{i}(0) y_{i}(\tau)\right] \mathrm{E}\left[x_{j}(0) y_{j}(\tau)\right] = 0, &  \text{if } i=k \neq j=\ell; \\
\mathrm{E}\left[x_{i}(0) y_{i}(\tau)\right] \mathrm{E}\left[x_{j}(0) y_{j}(\tau)\right] = 0, &  \text{if } i=\ell \neq j=k; \\
0, & \text{if } \text {else}.
\end{cases}\label{eqA17}
\end{align}

Replacing~\eqref{eqA15}, \eqref{eqA16} and~\eqref{eqA17} in~\eqref{eqA14}, we obtain:
\begin{equation}
\begin{split}
\lefteqn{\mathrm{E}\left[ I(0)I(\tau) \right]= {}}  \\
& 2 \mathrm{E}\left[N_{m}\right]\mathrm{E}\left[x_{i}^{2}(0)x_{i}^{2}(\tau)\right] \\
& {} + 2 \left\{\mathrm{E}\left[ N_{0}N_{\tau}\right]-\mathrm{E}\left[N_{m}\right]\right\}\mathrm{E} \left[x_{i}^{2}(0)\right] \mathrm{E}\left[x_{i}^{2}(\tau)\right] \\
& {}+ 4 \left\{\mathrm{E}\left[N_{m} N_{m}\right]-\mathrm{E}\left[N_{m}\right] \right\}\mathrm{E}\left[x_{i}(0)x_{i}(\tau)\right]\mathrm{E} \left[x_{i}(0)x_{i}(\tau)\right] \\
& {} + 2 \mathrm{E}\left[N_{m}\right]\mathrm{E}\left[x_{i}^{2}(0)\right] \mathrm{E}\left[y_{i}^{2}(\tau)\right] \\
& {} + 2 \left\{\mathrm{E}\left[N_{0}N_{\tau}\right]-\mathrm{E}\left[N_{m}\right] \right\}\mathrm{E}\left[x_{i}^{2}(0)\right] \mathrm{E}\left[y_{i}^{2}(\tau)\right],
\label{eqA18}
\end{split}
\end{equation}
where $N_{0} = N(0)$, $N_{\tau} = N(\tau)$, and $N_{m} = \min\{N_{0},N_{\tau}\}$.

According to~\cite{Papoulis:2002}:
\begin{equation}
\begin{split}
\mathrm{E}\left[x_{i}(0) x_{i}(\tau) \right] & = \rho_{x x}(\tau) \sigma_{x}^{2},  \text{ and}\\
\mathrm{E}\left[x_{i}^{2}(0) x_{i}^{2}(\tau) \right] & = \left[2\rho_{x x}^{2}(\tau) + 1\right] \sigma_{x}^{4}.
\label{eqA19}
\end{split}
\end{equation}

Substituting~\eqref{eqA19} in~\eqref{eqA18}, we have:
\begin{equation}
\begin{split}
\lefteqn{\mathrm{E}\left[ I(0)I(\tau) \right]} \\
& =   2 \mathrm{E}\left[N_{m}\right]\left[2\rho_{x x}^{2}(\tau) + 1\right] \sigma_{x}^{4} + 2 \left\{\mathrm{E}\left[ N_{0}N_{\tau}\right]-\mathrm{E}\left[N_{m}\right]\right\} \sigma_{x}^{4} \\
& {}+ 4 \left\{\mathrm{E}\left[N_{m} N_{m}\right]-\mathrm{E}\left[N_{m}\right] \right\} \rho_{x x}^{2}(\tau) \sigma_{x}^{4} \\
& {}+ 2 \mathrm{E}\left[N_{m}\right] \sigma_{x}^{4} + 2 \left\{\mathrm{E}\left[N_{0}N_{\tau}\right]-\mathrm{E}\left[N_{m}\right] \right\} \sigma_{x}^{4}.
\label{eqA20}
\end{split}
\end{equation}

Replacing~\eqref{eqA5}, \eqref{eqA13} and the following:
\begin{equation}
\mathrm{E}\left[ N_{0}N_{\tau}\right] = \rho_{N N}(\tau) \sigma_{N}^{2} + \mu_{N}^{2},
\label{eqA21}
\end{equation}
then, the autocorrelation coefficient $\rho_{I I}(\tau)$ of $I$ is:
\begin{equation}
\begin{split}
\rho_{I I}(\tau) & = \frac{\mathrm{E}[I(0) I(\tau)]-\mu_{I}^{2}}{\sigma_{I}^{2}} \\
& = \frac{\mathrm{E}\left[N_{m} N_{m}\right] \rho_{x x}^{2}(\tau)+\rho_{N N}(\tau) \sigma_{N}^{2}}{2 \sigma_{N}^{2}+\mu_{N}^{2}}.
\label{eqA22}
\end{split}
\end{equation}

%\ifCLASSOPTIONcaptionsoff
  \newpage
%\fi
\bibliography{references.bib}

\begin{sidewaystable*}
	\caption{Representation of eight single-point probability distributions by the proposed generalized framework}\label{Table1}
	\scriptsize
	\setlength{\tabcolsep}{2.5pt}
	\begin{tabular}{cccccccccccc}
		\toprule
		% after \\: \midrule or \cline{col1-col2} \cline{col3-col4} ...
		Distributions & Rayleigh & S$\upalpha$SGR & \multicolumn{3}{c}{$K$} & $G^0$ & $W$ & $U$  & Rician  &\multicolumn{2}{c}{RiIG} \\
		\cmidrule(lr){2-2}
		\cmidrule(lr){3-3}
		\cmidrule(lr){4-6}
		\cmidrule(lr){7-7}
		\cmidrule(lr){8-8}
		\cmidrule(lr){9-9}
		\cmidrule(lr){10-10}
		\cmidrule(lr){11-12}
		Parameters & $\sigma$,$s$,$N$ & $\alpha$,$\gamma$,$N$ & $\beta$,$\alpha$,$s$,$\bar{N}$ & $\beta$,$\alpha$,$s$,$\bar{\bar{N}}$ & $\beta$,$\alpha$,$N$ & $\gamma$,$\alpha$,$s$,$\bar{\bar{N}}$ & $\beta$,$p$,$q$,$s$,$\bar{\bar{N}}$ & $\beta$,$p$,$q$,$s$,$\bar{\bar{N}}$ & $\sigma$,$A_{0}$,$N$ & \makecell{$\alpha$,$\beta (\beta_{x},\beta_{y})$, \\ $\delta$,$\gamma$,$N$} & \makecell{$\alpha$,$\beta (\beta_{x},\beta_{y})$, \\ $\delta$,$\gamma$,$s$,$\bar{\bar{N}}$} \\
		\midrule
		$\mathcal{R}_{i}$ & $ \mathcal{N}\left(0,\frac{s^{2}}{N}\right)$ & $ S \alpha S\left(\alpha, \frac{\gamma}{\sqrt[\alpha]{N}}\right)$ &
		$ \mathcal{N}\left(0,\frac{s^{2}}{\bar{N}}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & $ \mathcal{N}\left(0,\frac{z}{2N}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & \makecell{$ \mathcal{N}\left(0,\frac{s^{2}}{N}\right)$ \\ $\mathcal{R}_{0} = A_{0}$}  & $ \mathcal{N}\left(\frac{\beta_{x}z}{N},\frac{z}{N}\right)$  & $ \mathcal{N}\left(\frac{\beta_{x}\delta}{\gamma \bar{\bar{N}}},\frac{s^{2}}{\bar{\bar{N}}}\right)$ \\
		\midrule
		$\mathcal{I}_{i}$ & $ \mathcal{N}\left(0,\frac{s^{2}}{N}\right)$ & $ S \alpha S\left(\alpha, \frac{\gamma}{\sqrt[\alpha]{N}}\right)$ &
		$ \mathcal{N}\left(0,\frac{s^{2}}{\bar{N}}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & $ \mathcal{N}\left(0,\frac{z}{2N}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & $ \mathcal{N}\left(0,\frac{s^{2}}{\bar{\bar{N}}}\right)$ & \makecell{$ \mathcal{N}\left(0,\frac{s^{2}}{N}\right)$ \\ $\mathcal{I}_{0} = 0$}  & $ \mathcal{N}\left(\frac{\beta_{y}z}{N},\frac{z}{N}\right)$  & $ \mathcal{N}\left(\frac{\beta_{y}\delta}{\gamma \bar{\bar{N}}},\frac{s^{2}}{\bar{\bar{N}}}\right)$ \\
		\midrule
		\makecell{Variance of \\ $\mathcal{R}_{i}$ and $\mathcal{I}_{i}$} & constant: $\frac{s^{2}}{N}$
		&
		$\left\{
		\begin{array}{lr}
		\frac{2\gamma^2}{N} &\text{if } \alpha=2,\\
		\infty		&\text{if } \alpha\neq 2.
		\end{array}
		\right.
		$
		& constant: $\frac{s^{2}}{\bar{N}}$ & constant: $\frac{s^{2}}{\bar{\bar{N}}}$ &\makecell{\text{variable: }$\frac{z}{2N}$; \\  Gamma \\ $p_{K}(z;\alpha,\beta)$ }& constant: $\frac{s^{2}}{\bar{\bar{N}}}$ & constant: $\frac{s^{2}}{\bar{\bar{N}}}$ & constant: $\frac{s^{2}}{\bar{\bar{N}}}$ & constant: $\frac{s^{2}}{N}$ & \makecell{ \text{variable: } $\frac{z}{2N}$; \\ IG \\ $p_{\text{RiIG}}(z;\delta,\gamma)$} & constant: $\frac{s^{2}}{\bar{\bar{N}}}$ \\
		\midrule
		$N$ & constant & constant & \makecell{Negative binomial \\ $p_{K}(N;\bar{N},\alpha)$} & \makecell{Poisson \\ $p(N;\bar{N})$} & constant & \makecell{Poisson \\ $p(N;\bar{N})$}& \makecell{Poisson \\ $p(N;\bar{N})$} & \makecell{Poisson \\ $p(N;\bar{N})$} & constant & constant & \makecell{Poisson \\ $p(N;\bar{N})$} \\
		\midrule
		$\operatorname{E}(N)$ & constant & constant & $\bar{N} \rightarrow \infty $ & \makecell{Gamma \\ $p_{K}(\bar{N};\alpha,\bar{\bar{N}})$} & constant & \makecell{IG \\ $p_{G^0}(\bar{N};\alpha, \bar{\bar{N}})$} & \makecell{Beta-I \\ $p_{W}(\bar{N};\beta,p,q)$} & \makecell{Beta-II \\ $p_{U}(\bar{N};\beta,p,q)$} & constant & constant & \makecell{IG \\ $ p_{\text{RiIG}}(\bar{N};\bar{\bar{N}},\gamma)$} \\
		\midrule
		Relationships & $\sigma = 2 s^{2}$ & -- & \multicolumn{2}{c}{$s = \sqrt{\frac{\alpha}{2\beta}}$} & -- & $s = \sqrt{\frac{\gamma}{2\alpha}}$ & $s = \sqrt{\frac{\beta p}{2(p+q)}}$ & $s = \sqrt{\frac{\beta p}{2(q-1)}}$ & $\sigma = 2 s^{2}$ &
		\multicolumn{2}{c}{
			$\begin{array}{lcl}
			\gamma &=& \sqrt{\alpha^2-\beta^2} \\
			\beta^2 &=& \beta_{x}^{2}+\beta_{y}^{2} \\
			s &=& \sqrt{\frac{\delta}{\gamma}}
			\end{array}
			$
		} \\
		\midrule
		$p(A)$ & $\frac{2A}{\sigma}e^{- \frac{A^{2}}{\sigma}}$ & $\makecell{ A \int_{0}^{\infty} w \exp \left[-\gamma w^{\alpha}\right] \cdot \\ J_{0}(w A) \mathrm{d} w}$ & \multicolumn{3}{c}{
			$\begin{array}{ll}
			&\frac{4 \beta A}{\Gamma(\alpha)}\left(\beta A^{2}\right)^{(\alpha+1) / 2-1} \cdot \\
			&K_{\alpha-1}(2 A \sqrt{\beta})
			\end{array}$
		} &
		$ \frac{2\alpha\gamma^{\alpha}A}{\Gamma(\alpha)(\gamma+A^{2})^{1+\alpha}}$ &
		$\begin{array}{ll}
		&\frac{2A\Gamma(p+q)}{\beta \Gamma(p)}\cdot \\
&(\frac{A^2}{\beta})^{\frac{p-2}{2}} \cdot \\
&\exp(-\frac{A^2}{2\beta})\cdot \\
		&W_{x,y}(\frac{A^2}{\beta}). \\
&(x = \frac{-p-2q+2}{2}, \\
& y = \frac{p-1}{2})
		\end{array}
		$
		& $\makecell{\frac{2A\Gamma(1+q)}{\beta B(p,q)} \cdot \\ U_{x,y}(\frac{A^2}{\beta}) \\
(x = q+1, \\ y = 2-p)}$ & $ \makecell{ \frac{2A}{\sigma} \cdot \\ \exp(-\frac{A^{2}+A_{0}^{2}}{\sigma}) \cdot \\ I_{0}(\frac{2AA_{0}}{\sigma})
		}$ &
		\multicolumn{2}{c}{$ \makecell{\sqrt{\frac{2}{\pi}}\alpha^{\frac{3}{2}}\exp(\delta\gamma)\cdot \\
\frac{A}{(\delta^{2}+A^{2})^{\frac{3}{4}}} \cdot \\
		K_{\frac{3}{2}}
		(\alpha \sqrt{\delta^2 + A^{2}})I_{0}(\beta A)
		}$} \\
		\midrule
		$p(\theta)$ & $ \frac{1}{2\pi}$ & $\frac{1}{2\pi}$ & \multicolumn{3}{c}{$ \frac{1}{2\pi}$} & $ \frac{1}{2\pi}$ & $ \frac{1}{2\pi}$ & $ \frac{1}{2\pi}$ &
		$\begin{array}{ll}
		&\frac{\exp \left\{-\frac{A_{0}^{2}}{\sigma}\right\}}{2 \pi} + \\ &\sqrt{\frac{1}{2 \pi}} \sqrt{\frac{2}{\sigma}} A_{0} \cdot\\
		&\exp \left\{-\frac{A_{0}^{2}\sin ^{2} \theta}{\sigma} \right\} \cdot\\ &\frac{1+\operatorname{erf}\left(\frac{A_{0} \cos \theta}{\sqrt{\sigma}}\right)}{2}
		\end{array}$
		&
		--
		\\
		\midrule
		Supplementary & \multicolumn{11}{l}{
			$\begin{array}{rcl} p(N;\bar{N}) & = & \frac{\bar{N}^{N}}{N!}\exp \{-\bar{N}\} \\
			p_{K}(N ; \bar{N}, \alpha) &=& {{N+\alpha-1}\choose{N}} \frac{(\bar{N} / \alpha)^{N}}{(1+\bar{N} / \alpha)^{N+\alpha}} \\
            p_{K}(\bar{N} ; \bar{\bar{N}}, \alpha) &=& \frac{1}{\Gamma(\alpha)}(\frac{\alpha}{\bar{\bar{N}}})^{\alpha}\bar{N}^{\alpha-1}\exp\{ - \frac{\bar{N}\alpha}{\bar{\bar{N}}}\} \\
			p_{K}(z;\alpha,\beta) &=& \frac{\beta^{\alpha}}{\Gamma(\alpha)} z^{\alpha-1} \exp \{-\beta z\} \\
			p_{G^{0}}(\bar{N};\alpha, \bar{\bar{N}}) &=& \frac{1}{\Gamma(\alpha)}(\frac{\alpha}{\bar{\bar{N}}})^{\alpha}\bar{N}^{-\alpha-1}\exp \{-\frac{\alpha}{\bar{\bar{N}}\bar{N}}\} \\
			p_{W}(\bar{N};\beta,p,q) &=& \frac{\beta^{1-p-q}}{B(p,q)}\bar{N}^{p-1}(\beta-\bar{N})^{q-1} \\
			p_U(\bar{N};\beta,p,q) &=& \frac{\beta^{q}}{B(p,q)}\frac{\bar{N}^{p-1}}{(\bar{N}+\beta)^{p+q}} \\
			p_{\mathrm{RiIG}}(z ; \delta, \gamma) &=& \frac{\delta}{2 \pi} \exp \{\delta \gamma\} z^{-3 / 2} \exp \{-\frac{1}{2}(\delta^{-2}{z}+\gamma^{2} z)\}\\
			p_{\mathrm{RiIG}}(\bar{N} ; \bar{\bar{N}}, \gamma) &=& \frac{\gamma \bar{\bar{N}}}{2 \pi} \exp \left\{\gamma^{2} \bar{\bar{N}}\right\} \bar{N}^{-3 / 2} \exp \left\{-\frac{1}{2}\left(\frac{\gamma^{2} \bar{\bar{N}}^{2}}{\bar{N}}+\gamma^{2} \bar{N}\right)\right\}
			\end{array}$
		}\\
		\bottomrule
	\end{tabular}
\end{sidewaystable*}

\end{document}


