---
title: "Lista de exercícios 5"
author: "Eduarda Chagas"
date: "August 15, 2019"
output: html_document
---

Neste trabalho iremos realizar a estimação dos parâmetros da lei GI0 de um conjunto de amostras de textura da banda HHHH de imagens PolSAR.
Para isto, seguiremos o passo a passo indicado no capítulo 6 do livro *Operational Statistics for SAR Imagery*.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Pacotes necessários

Durante a construção deste estudo fazemos necessária a utilização de alguns pacotes e funcionalidades já implementadas anteriormente, tornando assim o código mais limpo e o trabalho mais simples. 
Os pacotes e scripts utilizados foram os seguintes:


```{r}
source("../SAR/SARTimeSerie.R")
source("../Textures/TextureTimeSeries.R")
source("../Band&Pompe.R")
source("imagematrix.R")
require(maxLik)
require(stats4)
require(ggplot2)
require(ggpubr)
require(reshape2)
```


## Análise de uma única amostra 

Para o seguinte estudo utilizamos uma pequena amostra de $128 \times 128$ pixels de uma imagem do Parque Nacional Sierra del Lacandon, Guatemala (adquirido em 10 de abril de 2015),
disponível em [https://uavsar.jpl.nasa.gov/cgi-bin/product.pl?jobName=Lacand_30202_15043_
006_150410_L090_CX_01#dados](https://uavsar.jpl.nasa.gov/cgi-bin/product.pl?jobName=Lacand_30202_15043_
006_150410_L090_CX_01#dados). 
Para isto, primeiramente deveremos carregar os dados que serão utilizados.


```{r}
dimen.guatemala <- c(5150, 128, 2700, 128)
sar_data <- raster(paste("../../Data/", "guatemala", "/HHHH", ".grd", sep = ""))
img <- getValuesBlock(sar_data, row = dimen.guatemala[1], nrows = dimen.guatemala[2], col = dimen.guatemala[3], ncols = dimen.guatemala[4], format = "matrix")
plot(imagematrix(equalize(img)))
```

Na primeira linha carregamos a posição da amostra retirada da imagem e as dimensões que esta deve possuir, enquanto que na segunda e terceira linha carregamos tais dados na matrix $img$.
Por último, plotamos a imagem resultante.

Um resumo dos dados é mostrado a seguir e para isto, foi necessário realizar a linearização da matriz $img$.
Optamos neste estudo o uso das **curvas de Hilbert** para fazer esta transformação de representação dos dados, armazenando tais informações na variável $z$.

```{r}
z <- OneDimensionTextureHilbert(img)
z <- data.frame(z)
summary(z)
```

Como podemos observar, os dados são valores do tipo *float*, com $5$ casas decimais (logo, é esperado um número pequeno de valores repetidos ao longo do conjunto) e se encontram limitados no intervalo $[0, 1]$.

Para analisar o comportamento dos dados, contruímos os histogramas dos seus valores.
No primeiro gráfico, podemos enxergar que a grande maioria dos valores se encontram concentrados na esquerda, característica essa que realçada ainda mais quando calculamos o número de bins ($w$) utilizando a regra de Freedman-Diaconis, dada por: 

$$w = 2n^{-1/3}IQR(z)$$

onde IQR denota o intervalo interquartil, e n o número de observações.
Esse comportamento dos dados observado nos informa um comportamento típico das famílias de distribuição $G$. 

```{r}

binwidth_complete <- 2*IQR(z$z)*length(z$z)^(-1/3)

p1 <- ggplot(data = z) + 
  geom_histogram(aes(x = z, y = ..density..)) + 
  xlab("Intensities") +
  ylab("Proportions") +
  ggtitle("Histogram") +
  theme(plot.title = element_text(hjust=0.5))

p2 <- ggplot(data = z) + 
  geom_histogram(aes(x = z, y = ..density..), binwidth = binwidth_complete) + 
  xlab("Intensities") +
  ylab("Proportions") +
  ggtitle("Histogram with Freedman-Diaconis") +
  theme(plot.title = element_text(hjust=0.5))

ggarrange(p1, p2, ncol=2, nrow=1, common.legend = TRUE)
```

### Estimadores por Momentos e por Máxima Verossimilhança

Como todas as informações que possuímos sobre os dados são a amostra e o número de looks $L = 36$, iremos calcular um ponto inicial dos parâmetros $\breve{\alpha}$ e $\breve{\gamma}$ por meio da técnica de estimação por momentos e utilizar os resultados desta como entrada para a função de estimação por máxima verossimilhança.

Os valores de $\breve{\alpha}$ e $\breve{\gamma}$ iniciais são obtidos através da estimação por momentos por meio das seguintes equações:

$$\breve{\alpha} = -2 - \frac{L + 1}{L m_{2}/m_{1}^{2}}$$

$$\breve{\gamma} = m_1 \Big(2+\frac{L+1}{L\, m_2/m_1^2}\Big)$$

As respectivas funções são declaradas a seguir:

```{r}
looks = 36

GI0.Estimator.m1m2 <- function(z, L){
  m1 <- mean(z)
  m2 <- mean(z^2)
  m212 <- m2/m1^2
  
  a <- -2 - (L+1) / (L * m212)
  g <- m1 * (2 + (L+1) / (L * m212))
  
  return(list("alpha"=a, "gamma"=g))
}

GI0.Estimator.LogLikelihoodLknown <- function(params){
  p_alpha <- -abs(params[1])
  p_gamma <- abs(params[2])
  p_L <- abs(params[3])
  
  n <- length(z)
  
  return(
    n*(lgamma(p_L-p_alpha) - p_alpha*log(p_gamma) - lgamma(-p_alpha)) + 
      (p_alpha-p_L)*sum(log(p_gamma + z*p_L)) 
  )
}

```

Logo, os estimadores para a amostra utilizada são: 

```{r}
estimators.moments <- GI0.Estimator.m1m2(z$z,looks)
estimators.moments
```

Agora que temos os valores $(\breve{\alpha}, \breve{\gamma})$ como ponto inicial, podemos calcular a estimação por máxima verossimilhança, que no dado exemplo foi implementada com o auxílio da função *maxNR* do pacote *maxLik*, que realiza uma maximização baseada no método de aproximação quadrática (Newton).


```{r}
estimators.ML <- maxNR(GI0.Estimator.LogLikelihoodLknown, start=c(estimators.moments$alpha, estimators.moments$gamma, looks), activePar=c(TRUE,TRUE,FALSE))$estimate[1:2]
estimators.ML
```

Podemos notar que os valores de $(\widehat\alpha, \widehat\gamma)$ foram muito próximos dos valores iniciais obtidos pela estimação por momentos, apresentando assim um pequeno ajuste sobre esses valores já conhecidos.

No entanto, como já se é conhecido na literatura, o valor esperado para regiões com texturas moderadas se encontram no intervalo $\alpha \in [-6, -3]$. 
Apresentando assim, uma pequena inconsistência em nossa análise.

Podemos observar que tais valores conseguem modelar bem o comportamento geral dos dados e que não houveram diferenças evidentes entre os estimadores por momentos e por máxima verossimilhança.


```{r}
dGI0 <- function(z, p_alpha, p_gamma, p_Looks, log=FALSE) {
  if(log==TRUE) {
    return(
      (p_Looks*log(p_Looks) + lgamma(p_Looks-p_alpha) + (p_Looks-1)*log(z) ) - 
        (p_alpha*log(p_gamma) + lgamma(-p_alpha) + lgamma(p_Looks) + 
           (p_Looks-p_alpha)*log(p_gamma + z*p_Looks) ) 
    )   
  }
  else { return( 
    ( p_Looks^p_Looks * gamma(p_Looks-p_alpha) * z^(p_Looks-1) ) / 
      (p_gamma^p_alpha * gamma(-p_alpha) * gamma(p_Looks) * (p_gamma + z*p_Looks)^(p_Looks-p_alpha)) 
  )
  }
}

intensity <- seq(.001, 0.4, length.out = 5000)
dExp <- dexp(intensity, rate=1/mean(z$z))
dGI0Mom <- dGI0(intensity, estimators.moments$alpha, estimators.moments$gamma, looks)
dGI0ML <- dGI0(intensity, estimators.ML[1], estimators.ML[2], looks)
df.densities <- data.frame(intensity, dExp, dGI0Mom, dGI0ML)
densities.flat <- melt(df.densities, 
                       measure.vars = c("dExp", "dGI0Mom", "dGI0ML"),
                       variable_name = c("Intensity", "dExp", "dGI0Mom", "dGI0ML"))
names(densities.flat) <- c("Intensity", "Density", "value")

ggplot(data = z, aes(x=z)) + 
  geom_histogram(aes(y=..density..), col="white", binwidth = binwidth_complete) + 
  xlim(0,0.4) + 
  geom_line(data=densities.flat, aes(x=Intensity, y=value, col=Density)) + 
  xlab("Intensities from the Guatemala Area") + 
  ylab("Histogram, and fitted Exponential and G0 Laws") + 
  ggtitle("Restricted Histogram and fitted densities") +
  theme(plot.title = element_text(hjust=0.5))
```

## Análise do conjunto de amostras utilizadas no estudo

Para esta análise, três imagens SAR com diferentes regiões foram usadas, são elas:

* Parque Nacional Sierra del Lacandon, Guatemala (adquirido em 10 de abril de 2015), disponível em [https://uavsar.jpl.nasa.gov/cgi-bin/product.pl?jobName=Lacand_30202_15043_
006_150410_L090_CX_01#dados](https://uavsar.jpl.nasa.gov/cgi-bin/product.pl?jobName=Lacand_30202_15043_
006_150410_L090_CX_01#dados);

* Regiões oceânicas do Cabo Canaveral (adquirido em 22 de setembro de 2016);

* Área urbana da cidade de Munique, na Alemanha (adquirido em 5 de junho de 2015).

Um total de 160 amostras foram consideradas durante a investigação, sendo 40 amostras de cada categoria de regiões, são elas: regiões florestais da Guatemala; regiões oceânicas de Cape Canaveral com comportamento 1; regiões oceânicas de Cape Canaveral com comportamento 2 e regiões urbanas da cidade de Munique.

Logo, iremos inicialmente definir esse conjunto de amostras que será utilizada ao longo da análise:

```{r}
ns = 40
ns.guatemala = 40
dimen.guatemala <- matrix(nrow = ns.guatemala, ncol = 4)

ns.canaveral.behavior1 = 40
dimen.canaveral.behavior1 <- matrix(nrow = ns.canaveral.behavior1, ncol = 4)

ns.canaveral.behavior2 = 40
dimen.canaveral.behavior2 <- matrix(nrow = ns.canaveral.behavior2, ncol = 4)

ns.munich = 40
dimen.munich <- matrix(nrow = ns.munich, ncol = 4)

#Ocean regions in Cape Canaveral
row1 <- c(50, 100, 150, 200, 250, 350, 450, 550, 650, 750)
row2 <- c(50, 100, 150, 200, 250, 300, 350, 400, 450, 550)
row3 <- c(50, 150, 250, 350, 450, 550, 650, 750, 800, 850)
row4 <- c(250, 350, 450, 550, 650, 750, 850, 950, 1050)
row5 <- c(50, 150, 250, 350, 450, 550, 650, 750, 850, 950, 1050)
row6 <- c(50, 150, 250, 350, 450, 550, 650, 750, 800, 850, 950)
row7 <- c(50, 150, 250, 350, 450, 550, 650, 750, 850, 950)
cols <- c(1700, 1850, 1550, 1400, 1, 200, 350, 550)
#{Behavior 1}
dimen.canaveral.behavior1[1:10,] <- c(row1, rep(128, 10), rep(cols[1], 10), rep(128, 10))
dimen.canaveral.behavior1[11:20,] <- c(row2, rep(128, 10), rep(cols[2], 10), rep(128, 10))
dimen.canaveral.behavior1[21:30,] <- c(row3, rep(128, 10), rep(cols[3], 10), rep(128, 10))
dimen.canaveral.behavior1[31:40,] <- c(row3, rep(128, 10), rep(cols[4], 10), rep(128, 10))
#{Behavior 2}
dimen.canaveral.behavior2[1:9,] <- c(row4, rep(128, 9), rep(cols[5], 9), rep(128, 9))
dimen.canaveral.behavior2[10:20,] <- c(row5, rep(128, 11), rep(cols[6], 11), rep(128, 11))
dimen.canaveral.behavior2[21:30,] <- c(row7, rep(128, 10), rep(cols[7], 10), rep(128, 10))
dimen.canaveral.behavior2[31:40,] <- c(row7, rep(128, 10), rep(cols[8], 10), rep(128, 10))

#Urban regions in Munich
row1 <- seq(3000, 3950, by = 50)
row2 <- rep(c(4300, 4350), 5)
row3 <- c(rep(seq(2300, 2450, by = 50), 2), 2400, 2450)
cols <- 400
cols2 <- c(1300, 1300, 1350, 1350, 1400, 1400, 1450, 1450, 1500, 1500)
cols3 <- c(rep(500, 4), rep(400, 4), rep(300, 2))
dimen.munich[1:20,] <- c(row1, rep(128, 20), rep(cols, 20), rep(128, 20))
dimen.munich[21:30,] <- c(row2, rep(128, 10), cols2, rep(128, 10))
dimen.munich[31:40,] <- c(row3, rep(128, 10), cols3, rep(128, 10))

#Forest regions in Guatemala
row1 <- seq(5150, 6100, by = 50)
row2 <- seq(5200, 5650, by = 50)
row3 <- seq(4100, 4200, by = 50)
row4 <- seq(1000, 1150, by = 50)
cols <- c(2700, 2800, 2930, 1930, 1870)
dimen.guatemala[1:20,] <- c(row1, rep(128, 20), rep(cols[1], 20), rep(128, 20))
dimen.guatemala[21:30,] <- c(row2, rep(128, 10), rep(cols[2], 10), rep(128, 10))
dimen.guatemala[31:33,] <- c(row3, rep(128, 3), rep(cols[3], 3), rep(128, 3))
dimen.guatemala[34:37,] <- c(row4, rep(128, 4), rep(cols[4], 4), rep(128, 4))
dimen.guatemala[38:40,] <- c(row4[1:3], rep(128, 3), rep(cols[5], 3), rep(128, 3))

```

Para facilitar o processo de geração dos estimadores foi construída a função $SAR.Analysis.Regions$, que de acordo com a opção selecionada, analisa um conjunto das amostras pré-determinado.

```{r}
SAR.Analysis.Regions <- function(option){
  
  estimators.ML = matrix(ncol = 2, nrow = ns)
  estimators.moments = array(list(), ns)
  looks = 36
  
  if(option == 1){ #Forest regions in Guatemala
    sar_data <- raster(paste("../../Data/", "guatemala", "/HHHH", ".grd", sep = ""))
    for(j in c(1:ns.guatemala)){
        img <- getValuesBlock(sar_data, row = dimen.guatemala[j,1], nrows = dimen.guatemala[j,2], col = dimen.guatemala[j,3], ncols = dimen.guatemala[j,4], format = "matrix")
        z <- OneDimensionTextureHilbert(img)
        estimators.moments[[j]] <- GI0.Estimator.m1m2(z,looks)
        estimators.ML[j,] <- maxNR(GI0.Estimator.LogLikelihoodLknown, start=c(estimators.moments[[j]]$alpha, estimators.moments[[j]]$gamma, looks), activePar=c(TRUE,TRUE,FALSE))$estimate[1:2]
    }
  }
  else if(option == 2){ #Cape Canaveral - behavior 1
    sar_data <- raster(paste("../../Data/", "cape", "/HHHH", ".grd", sep = ""))
    for(j in c(1:ns.canaveral.behavior1)){
        img <- getValuesBlock(sar_data, row = dimen.canaveral.behavior1[j,1], nrows = dimen.canaveral.behavior1[j,2], col = dimen.canaveral.behavior1[j,3], ncols = dimen.canaveral.behavior1[j,4], format = "matrix")
        z <- OneDimensionTextureHilbert(img)
        estimators.moments[[j]] <- GI0.Estimator.m1m2(z,looks)
        estimators.ML[j,] <- maxNR(GI0.Estimator.LogLikelihoodLknown, start=c(estimators.moments[[j]]$alpha, estimators.moments[[j]]$gamma, looks), activePar=c(TRUE,TRUE,FALSE))$estimate[1:2]
    }
  }
  else if(option == 3){ #Cape Canaveral - behavior 2
    sar_data <- raster(paste("../../Data/", "cape", "/HHHH", ".grd", sep = ""))
    for(j in c(1:ns.canaveral.behavior2)){
        img <- getValuesBlock(sar_data, row = dimen.canaveral.behavior2[j,1], nrows = dimen.canaveral.behavior2[j,2], col = dimen.canaveral.behavior2[j,3], ncols = dimen.canaveral.behavior2[j,4], format = "matrix")
        z <- OneDimensionTextureHilbert(img)
        estimators.moments[[j]] <- GI0.Estimator.m1m2(z,looks)
        estimators.ML[j,] <- maxNR(GI0.Estimator.LogLikelihoodLknown, start=c(estimators.moments[[j]]$alpha, estimators.moments[[j]]$gamma, looks), activePar=c(TRUE,TRUE,FALSE))$estimate[1:2]
    }
  }
  else{ #Urban regions in Munich
    sar_data <- raster(paste("../../Data/", "munich", "/HHHH", ".grd", sep = ""))
    for(j in c(1:ns.munich)){
        img <- getValuesBlock(sar_data, row = dimen.munich[j,1], nrows = dimen.munich[j,2], col = dimen.munich[j,3], ncols = dimen.munich[j,4], format = "matrix")
        z <- OneDimensionTextureHilbert(img)
        estimators.moments[[j]] <- GI0.Estimator.m1m2(z,looks)
        estimators.ML[j,] <- maxNR(GI0.Estimator.LogLikelihoodLknown, start=c(estimators.moments[[j]]$alpha, estimators.moments[[j]]$gamma, looks), activePar=c(TRUE,TRUE,FALSE))$estimate[1:2]
    }
  }
  parameters <- data.frame(alpha.moments = estimators.moments[[j]]$alpha, gamma.moments = estimators.moments[[j]]$gamma, alpha.ML = estimators.ML[,1], gamma.ML = estimators.ML[,2])
  parameters
}
```

Entretanto, embora o procedimento seguido aqui tenha sido o mesmo realizado na análise de uma única amostra, os resultados obtidos são confusos, demonstrando que algum erro deve ter ocorrido ao longo do processo.
Para ilustrar isto iremos analisar individualmente cada conjunto de amostras e seus respectivos resultados.

### Análise dos dados

O primeiro conjunto trata-se das regiões florestais da Guatemala.

```{r}
Guatemala = SAR.Analysis.Regions(1)
Guatemala
```


Como podemos ver, existem até mesmo valores positivos de $\alpha$.
Analisando o resumo de tais valores, observamos que existe uma pequena variação entre eles.

```{r}
summary(Guatemala)
```

Já ao analisar o conjunto de amostras da região oceânica de Cape que possui o intitulado "comportamento 1", temos um cenário semelhante que se repete também nos demais conjuntos:

```{r}
Cape1 = SAR.Analysis.Regions(2)
summary(Cape1)
```

Desse modo, concluímos com a análise que o estudo dos dados não apresentou resultados, uma vez que erros foram apresentados, impossibilitando o continuamento do trabalho.